{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0b69dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdb716de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv('bank-additional-full.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd58cc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e3d6b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['job', 'marital', 'education', 'default', 'housing', 'loan',\n",
    "    'contact', 'month', 'day_of_week', 'poutcome']\n",
    "\n",
    "numerical_columns = ['age', 'duration', 'campaign', 'pdays', 'previous', 'emp.var.rate', \n",
    "    'cons.price.idx', 'cons.conf.idx', 'euribor3m', 'nr.employed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47fac5f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'job': 12,\n",
       " 'marital': 4,\n",
       " 'education': 8,\n",
       " 'default': 3,\n",
       " 'housing': 3,\n",
       " 'loan': 3,\n",
       " 'contact': 2,\n",
       " 'month': 10,\n",
       " 'day_of_week': 5,\n",
       " 'poutcome': 3}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_category = {}\n",
    "\n",
    "for item in categorical_columns:\n",
    "       count_value = 0\n",
    "       count_list = df[item].value_counts().to_numpy()\n",
    "       for cat in count_list: \n",
    "              count_value += 1\n",
    "       num_category[item] = count_value\n",
    "       \n",
    "num_category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d55a3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32059512",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "df['labels'] = label_encoder.fit_transform(df['y'])\n",
    "\n",
    "X = df.drop(columns=['y', 'labels'])\n",
    "y = df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4d6eba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "100f1cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('one_hot_encoder', OneHotEncoder(handle_unknown='ignore'), categorical_columns),\n",
    "        ('standard_scaler', StandardScaler(), numerical_columns)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa58025",
   "metadata": {},
   "source": [
    "## <b>Logistic Regression Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8d257dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Best Logistic Regression score : 0.7511651087481637\n",
      "Best Logistic Regression Parametes : \n",
      " {'logistic_regression__solver': 'liblinear', 'logistic_regression__penalty': 'l2', 'logistic_regression__max_iter': 500, 'logistic_regression__class_weight': 'balanced', 'logistic_regression__C': 10}\n",
      "Accuracy Score : 0.8625880067977665\n",
      "Confusion Matrix : \n",
      "[[9406 1559]\n",
      " [ 139 1253]]\n",
      "classification Report : \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92     10965\n",
      "           1       0.45      0.90      0.60      1392\n",
      "\n",
      "    accuracy                           0.86     12357\n",
      "   macro avg       0.72      0.88      0.76     12357\n",
      "weighted avg       0.92      0.86      0.88     12357\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logistic_pipeline = Pipeline(\n",
    "    steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('logistic_regression', LogisticRegression())\n",
    "    ]\n",
    ")\n",
    "\n",
    "logistic_param = logistic_param = {\n",
    "    'logistic_regression__solver': ['liblinear', 'saga'],\n",
    "    'logistic_regression__penalty': ['l1', 'l2'],\n",
    "    'logistic_regression__C': [0.01, 0.1, 1, 10],\n",
    "    'logistic_regression__max_iter': [500, 1000],\n",
    "    'logistic_regression__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "\n",
    "logistic_search = RandomizedSearchCV(\n",
    "    estimator=logistic_pipeline,\n",
    "    param_distributions=logistic_param,\n",
    "    cv=5,\n",
    "    n_iter=10,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    scoring='f1_macro'\n",
    ")\n",
    "\n",
    "logistic_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best Logistic Regression score : {logistic_search.best_score_}\")\n",
    "print(f\"Best Logistic Regression Parametes : \\n {logistic_search.best_params_}\")\n",
    "\n",
    "best_logistic = logistic_search.best_estimator_\n",
    "\n",
    "logistic_pred = best_logistic.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy Score : {accuracy_score(y_test, logistic_pred)}\")\n",
    "print(f\"Confusion Matrix : \\n{confusion_matrix(y_test, logistic_pred)}\")\n",
    "print(f\"classification Report : \\n{classification_report(y_test, logistic_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8e595a",
   "metadata": {},
   "source": [
    "## <b>XGBoost Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd6f6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "🎯 Best XGBoost Params: {'classifier__colsample_bytree': np.float64(0.7824279936868144), 'classifier__gamma': np.float64(3.925879806965068), 'classifier__learning_rate': np.float64(0.04993475643167195), 'classifier__max_depth': 6, 'classifier__min_child_weight': 1, 'classifier__n_estimators': 330, 'classifier__subsample': np.float64(0.9439761626945282)}\n",
      "\n",
      "📊 XGBoost (Tuned) Results:\n",
      "Accuracy: 0.9186695799951444\n",
      "F1: 0.7759814151007809\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.97      0.95     10965\n",
      "           1       0.68      0.54      0.60      1392\n",
      "\n",
      "    accuracy                           0.92     12357\n",
      "   macro avg       0.81      0.75      0.78     12357\n",
      "weighted avg       0.91      0.92      0.91     12357\n",
      "\n",
      "[[10607   358]\n",
      " [  647   745]]\n"
     ]
    }
   ],
   "source": [
    "xgb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric='logloss',\n",
    "        use_label_encoder=False\n",
    "    ))\n",
    "])\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'classifier__n_estimators': randint(200, 600),\n",
    "    'classifier__learning_rate': uniform(0.01, 0.2),\n",
    "    'classifier__max_depth': randint(3, 8),\n",
    "    'classifier__subsample': uniform(0.6, 0.4),\n",
    "    'classifier__colsample_bytree': uniform(0.6, 0.4),\n",
    "    'classifier__gamma': uniform(0, 5),\n",
    "    'classifier__min_child_weight': randint(1, 6)\n",
    "}\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_pipeline,\n",
    "    xgb_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_search.fit(X_train, y_train)\n",
    "print(\"\\n🎯 Best XGBoost Params:\", xgb_search.best_params_)\n",
    "\n",
    "xgb_best = xgb_search.best_estimator_\n",
    "y_pred_xgb = xgb_best.predict(X_test)\n",
    "print(\"\\n📊 XGBoost (Tuned) Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_xgb, average='macro'))\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "print(confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba5d6b1",
   "metadata": {},
   "source": [
    "## <b>Gradient Boosting Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b97016b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "🎯 Best Gradient Boosting Params: {'classifier__learning_rate': np.float64(0.05158833257363777), 'classifier__max_depth': 5, 'classifier__min_samples_split': 7, 'classifier__n_estimators': 290, 'classifier__subsample': np.float64(0.9526854323784995)}\n",
      "\n",
      "📊 Gradient Boosting (Tuned) Results:\n",
      "Accuracy: 0.9191551347414421\n",
      "F1: 0.7827113496866913\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.95     10965\n",
      "           1       0.67      0.56      0.61      1392\n",
      "\n",
      "    accuracy                           0.92     12357\n",
      "   macro avg       0.81      0.76      0.78     12357\n",
      "weighted avg       0.91      0.92      0.92     12357\n",
      "\n",
      "[[10575   390]\n",
      " [  609   783]]\n"
     ]
    }
   ],
   "source": [
    "gb_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GradientBoostingClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "gb_param_dist = {\n",
    "    'classifier__n_estimators': randint(100, 400),\n",
    "    'classifier__learning_rate': uniform(0.01, 0.2),\n",
    "    'classifier__max_depth': randint(2, 6),\n",
    "    'classifier__subsample': uniform(0.7, 0.3),\n",
    "    'classifier__min_samples_split': randint(2, 10)\n",
    "}\n",
    "\n",
    "gb_search = RandomizedSearchCV(\n",
    "    gb_pipeline,\n",
    "    gb_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "gb_search.fit(X_train, y_train)\n",
    "print(\"\\n🎯 Best Gradient Boosting Params:\", gb_search.best_params_)\n",
    "\n",
    "gb_best = gb_search.best_estimator_\n",
    "y_pred_gb = gb_best.predict(X_test)\n",
    "print(\"\\n📊 Gradient Boosting (Tuned) Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_gb, average='macro'))\n",
    "print(classification_report(y_test, y_pred_gb))\n",
    "print(confusion_matrix(y_test, y_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07429bac",
   "metadata": {},
   "source": [
    "## <b>Random Forest Model</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ef353b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n",
      "\n",
      "🎯 Best Random Forest Params: {'classifier__bootstrap': True, 'classifier__max_depth': 7, 'classifier__max_features': None, 'classifier__min_samples_leaf': 1, 'classifier__min_samples_split': 2, 'classifier__n_estimators': 266}\n",
      "\n",
      "📊 Random Forest (Tuned) Results:\n",
      "Accuracy: 0.9206117989803351\n",
      "F1: 0.7848442623906641\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     10965\n",
      "           1       0.68      0.56      0.61      1392\n",
      "\n",
      "    accuracy                           0.92     12357\n",
      "   macro avg       0.81      0.76      0.78     12357\n",
      "weighted avg       0.92      0.92      0.92     12357\n",
      "\n",
      "[[10596   369]\n",
      " [  612   780]]\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "rf_param_dist = {\n",
    "    'classifier__n_estimators': randint(100, 500),\n",
    "    'classifier__max_depth': randint(5, 20),\n",
    "    'classifier__min_samples_split': randint(2, 10),\n",
    "    'classifier__min_samples_leaf': randint(1, 5),\n",
    "    'classifier__max_features': ['sqrt', 'log2', None],\n",
    "    'classifier__bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf_pipeline,\n",
    "    rf_param_dist,\n",
    "    n_iter=20,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    random_state=42,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_search.fit(X_train, y_train)\n",
    "print(\"\\n🎯 Best Random Forest Params:\", rf_search.best_params_)\n",
    "\n",
    "rf_best = rf_search.best_estimator_\n",
    "y_pred_rf = rf_best.predict(X_test)\n",
    "print(\"\\n📊 Random Forest (Tuned) Results:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
    "print(\"F1:\", f1_score(y_test, y_pred_rf, average='macro'))\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "print(confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa069342",
   "metadata": {},
   "source": [
    "## <b>More Classification Models</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d618ec",
   "metadata": {},
   "source": [
    "You’ve already covered the “core four” for structured/tabular data:\n",
    "\n",
    "* **Logistic Regression** → linear baseline\n",
    "* **Random Forest** → bagging-based ensemble\n",
    "* **Gradient Boosting / XGBoost** → boosting-based ensembles\n",
    "\n",
    "But there are several **other strong classifiers** you can try — some classical, some modern. Here’s a quick breakdown by category 👇\n",
    "\n",
    "---\n",
    "\n",
    "## ⚙️ 1. Classic ML Models (fast and interpretable)\n",
    "\n",
    "### 🔸 **Support Vector Machine (SVM)**\n",
    "\n",
    "Excellent for smaller or medium datasets with clear margins between classes.\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', SVC(probability=True, random_state=42))\n",
    "])\n",
    "\n",
    "svm_param_dist = {\n",
    "    'classifier__C': [0.1, 1, 10],\n",
    "    'classifier__kernel': ['linear', 'rbf', 'poly'],\n",
    "    'classifier__gamma': ['scale', 'auto']\n",
    "}\n",
    "```\n",
    "\n",
    "✅ Pros: Handles non-linear boundaries with kernel trick\n",
    "⚠️ Cons: Slower on very large datasets (~40K rows might be borderline)\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 **K-Nearest Neighbors (KNN)**\n",
    "\n",
    "Simple, non-parametric, distance-based method.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "knn_param_dist = {\n",
    "    'classifier__n_neighbors': [3, 5, 7, 9],\n",
    "    'classifier__weights': ['uniform', 'distance'],\n",
    "    'classifier__p': [1, 2]  # 1=Manhattan, 2=Euclidean\n",
    "}\n",
    "```\n",
    "\n",
    "✅ Pros: Easy to interpret, no training time\n",
    "⚠️ Cons: Slow prediction on large datasets\n",
    "\n",
    "---\n",
    "\n",
    "## 🌳 2. Tree-based Boosting Variants\n",
    "\n",
    "### 🔸 **LightGBM**\n",
    "\n",
    "Highly optimized gradient boosting (faster than XGBoost on large tabular data).\n",
    "\n",
    "```python\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LGBMClassifier(random_state=42))\n",
    "])\n",
    "```\n",
    "\n",
    "✅ Pros: Very fast, handles categorical features natively (if passed correctly)\n",
    "⚠️ Cons: Slightly more sensitive to tuning\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 **CatBoost**\n",
    "\n",
    "Performs extremely well with categorical features — often best out-of-the-box.\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', CatBoostClassifier(\n",
    "        verbose=0, random_state=42\n",
    "    ))\n",
    "])\n",
    "```\n",
    "\n",
    "✅ Pros: Handles categorical features and missing values automatically\n",
    "⚠️ Cons: Slower to train than LightGBM, but great accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 3. Advanced / Hybrid Models\n",
    "\n",
    "### 🔸 **Naive Bayes**\n",
    "\n",
    "Quick baseline for text-like or categorical-heavy data.\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "nb_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', GaussianNB())\n",
    "])\n",
    "```\n",
    "\n",
    "✅ Pros: Very fast, interpretable\n",
    "⚠️ Cons: Strong independence assumptions — weaker on correlated features\n",
    "\n",
    "---\n",
    "\n",
    "### 🔸 **MLP (Neural Network)**\n",
    "\n",
    "Scikit-learn’s `MLPClassifier` gives a shallow neural net baseline.\n",
    "\n",
    "```python\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                                 activation='relu',\n",
    "                                 solver='adam',\n",
    "                                 max_iter=500,\n",
    "                                 random_state=42))\n",
    "])\n",
    "```\n",
    "\n",
    "✅ Pros: Can capture non-linear relationships\n",
    "⚠️ Cons: Slower to tune, sensitive to scaling and regularization\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Recommended Steps\n",
    "\n",
    "1. **LightGBM** — fast, strong, robust\n",
    "2. **CatBoost** — especially great for categorical-heavy datasets like yours\n",
    "3. **SVM** — if you want a strong linear/nonlinear boundary benchmark\n",
    "4. **MLP** — if you want a lightweight neural approach\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
