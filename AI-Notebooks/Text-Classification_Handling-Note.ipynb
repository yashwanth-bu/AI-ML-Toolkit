{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29a2f31",
   "metadata": {},
   "source": [
    "Perfect! Thanks for confirming the dataset sizes:\n",
    "\n",
    "* **Train:** 40,000 samples\n",
    "* **Valid:** 5,000 samples\n",
    "* **Test:** 5,000 samples\n",
    "\n",
    "We can now **proceed step by step** using your **keywords-aware spaCy cleaning** and build a full **TensorFlow sentiment analysis pipeline**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Import libraries and load data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load CSV files\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train :\", train_df.shape, \"Valid :\", valid_df.shape, \"Test :\", test_df.shape)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Load keywords and customize stopwords\n",
    "\n",
    "```python\n",
    "# Load keywords\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [w.strip() for w in f.read().split(\",\") if w.strip()]\n",
    "\n",
    "# Load spaCy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Customize stopwords: remove keywords from default stopwords\n",
    "stopwords = STOP_WORDS - set(keywords)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Define spaCy cleaning function\n",
    "\n",
    "```python\n",
    "def spacy_clean_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.text not in stopwords    # remove stopwords except keywords\n",
    "        and not token.is_punct            # remove punctuation\n",
    "        and token.is_alpha                # keep only alphabetic tokens\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(spacy_clean_text)\n",
    "valid_df[\"clean_text\"] = valid_df[\"text\"].apply(spacy_clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(spacy_clean_text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Tokenize and pad sequences\n",
    "\n",
    "```python\n",
    "max_words = 10000   # vocabulary size\n",
    "max_len = 200       # max tokens per sample\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "X_valid = tokenizer.texts_to_sequences(valid_df[\"clean_text\"])\n",
    "X_test = tokenizer.texts_to_sequences(test_df[\"clean_text\"])\n",
    "\n",
    "# Pad sequences\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_valid = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len)\n",
    "X_test = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "# Labels\n",
    "y_train = train_df[\"label\"].values\n",
    "y_valid = valid_df[\"label\"].values\n",
    "y_test = test_df[\"label\"].values\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Build the LSTM model\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=max_words, output_dim=64, input_length=max_len),\n",
    "    layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Train the model\n",
    "\n",
    "```python\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Evaluate on test set\n",
    "\n",
    "```python\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Predict new sentences\n",
    "\n",
    "```python\n",
    "def predict_sentiment(text):\n",
    "    cleaned = spacy_clean_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    sentiment = \"üòä Positive\" if pred > 0.5 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred:.3f} ‚Üí {sentiment}\")\n",
    "\n",
    "predict_sentiment(\"I absolutely loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"This film was boring and terrible, I hated it.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üí° At this stage you have:\n",
    "\n",
    "* Custom **keywords-aware cleaning**\n",
    "* Tokenization and padding\n",
    "* **LSTM-based sentiment analysis model**\n",
    "* Training, validation, and test evaluation\n",
    "\n",
    "---\n",
    "\n",
    "If you want, the **next step** could be:\n",
    "\n",
    "1. **Visualize training curves** for loss/accuracy.\n",
    "2. **Upgrade the model** to **Bidirectional LSTM** or **CNN+LSTM** for better performance.\n",
    "\n",
    "Do you want me to do that next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348a7098",
   "metadata": {},
   "source": [
    "Absolutely! Here‚Äôs a **full, end-to-end TensorFlow sentiment analysis pipeline** using:\n",
    "\n",
    "* **SpaCy cleaning with keywords-aware stopwords**\n",
    "* **Tokenization and padding**\n",
    "* **N-gram augmentation**\n",
    "* **Bidirectional LSTM with Dense layers**\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Import libraries\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train :\", train_df.shape, \"Valid :\", valid_df.shape, \"Test :\", test_df.shape)\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Load keywords and customize stopwords\n",
    "# ==========================\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [w.strip() for w in f.read().split(\",\") if w.strip()]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = STOP_WORDS - set(keywords)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ SpaCy text cleaning\n",
    "# ==========================\n",
    "def spacy_clean_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.text not in stopwords   # remove stopwords except keywords\n",
    "        and not token.is_punct           # remove punctuation\n",
    "        and token.is_alpha               # keep only alphabetic tokens\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply cleaning\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(spacy_clean_text)\n",
    "valid_df[\"clean_text\"] = valid_df[\"text\"].apply(spacy_clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(spacy_clean_text)\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Tokenization & padding\n",
    "# ==========================\n",
    "max_words = 10000\n",
    "max_len = 200\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_df[\"clean_text\"])\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "X_valid = tokenizer.texts_to_sequences(valid_df[\"clean_text\"])\n",
    "X_test  = tokenizer.texts_to_sequences(test_df[\"clean_text\"])\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_valid = valid_df[\"label\"].values\n",
    "y_test  = test_df[\"label\"].values\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ N-gram augmentation\n",
    "# ==========================\n",
    "def create_ngrams(sequence, n=2):\n",
    "    ngrams = []\n",
    "    for i in range(len(sequence) - n + 1):\n",
    "        ngram = tuple(sequence[i:i+n])\n",
    "        ngrams.append(ngram)\n",
    "    return ngrams\n",
    "\n",
    "def add_ngrams(sequences, n=2):\n",
    "    new_sequences = []\n",
    "    max_index = max([max(seq) if len(seq) > 0 else 0 for seq in sequences]) + 1\n",
    "    for seq in sequences:\n",
    "        seq_ngrams = []\n",
    "        ngram_tuples = create_ngrams(seq, n)\n",
    "        for ng in ngram_tuples:\n",
    "            ng_id = sum([w*(max_index**i) for i,w in enumerate(ng)])\n",
    "            seq_ngrams.append(ng_id)\n",
    "        new_sequences.append(seq + seq_ngrams)\n",
    "    return new_sequences\n",
    "\n",
    "X_train_ngram = add_ngrams(X_train, n=2)\n",
    "X_valid_ngram = add_ngrams(X_valid, n=2)\n",
    "X_test_ngram  = add_ngrams(X_test, n=2)\n",
    "\n",
    "# Pad sequences\n",
    "X_train_ngram = keras.preprocessing.sequence.pad_sequences(X_train_ngram, maxlen=max_len)\n",
    "X_valid_ngram = keras.preprocessing.sequence.pad_sequences(X_valid_ngram, maxlen=max_len)\n",
    "X_test_ngram  = keras.preprocessing.sequence.pad_sequences(X_test_ngram, maxlen=max_len)\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Build Bidirectional LSTM model\n",
    "# ==========================\n",
    "model = keras.Sequential([\n",
    "    layers.Embedding(input_dim=max_words*4,  # bigger vocab for n-grams\n",
    "                     output_dim=128,\n",
    "                     input_length=max_len),\n",
    "    layers.Bidirectional(layers.LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Train model\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    X_train_ngram, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid_ngram, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Evaluate model\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(X_test_ngram, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# üîü Predict new text\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    cleaned = spacy_clean_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    seq_ngram = add_ngrams(seq, n=2)\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(seq_ngram, maxlen=max_len)\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    sentiment = \"üòä Positive\" if pred > 0.5 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred:.3f} ‚Üí {sentiment}\")\n",
    "\n",
    "# Example\n",
    "predict_sentiment(\"I really loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"Terrible film, I wasted my time.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This code covers the **full pipeline**:\n",
    "\n",
    "* Keywords-aware cleaning\n",
    "* Tokenization + padding\n",
    "* N-grams for phrase context\n",
    "* Bidirectional LSTM\n",
    "* Dense layers + sigmoid output\n",
    "* Training, evaluation, and new predictions\n",
    "\n",
    "---\n",
    "\n",
    "I can also provide a **version with attention and pretrained embeddings** to further boost performance if you want.\n",
    "\n",
    "Do you want me to do that next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffb0a05",
   "metadata": {},
   "source": [
    "Perfect! Let‚Äôs upgrade your **sentiment analysis pipeline** to use:\n",
    "\n",
    "1. **Pre-trained embeddings (GloVe)** ‚Äì to capture semantic meaning from a large corpus.\n",
    "2. **Global vocabulary** ‚Äì to cover all words across train/valid/test.\n",
    "3. **Attention layer** ‚Äì to let the model focus on important words in a sentence.\n",
    "4. **Bidirectional LSTM** ‚Äì for context in both directions.\n",
    "\n",
    "We‚Äôll do this **step by step** in one full code block.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Import libraries\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train :\", train_df.shape, \"Valid :\", valid_df.shape, \"Test :\", test_df.shape)\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Load keywords and customize stopwords\n",
    "# ==========================\n",
    "with open(\"keywords.txt\", \"r\") as f:\n",
    "    keywords = [w.strip() for w in f.read().split(\",\") if w.strip()]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = STOP_WORDS - set(keywords)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ SpaCy cleaning\n",
    "# ==========================\n",
    "def spacy_clean_text(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [\n",
    "        token.lemma_\n",
    "        for token in doc\n",
    "        if token.text not in stopwords\n",
    "        and not token.is_punct\n",
    "        and token.is_alpha\n",
    "    ]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(spacy_clean_text)\n",
    "valid_df[\"clean_text\"] = valid_df[\"text\"].apply(spacy_clean_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(spacy_clean_text)\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Tokenization & Global vocab\n",
    "# ==========================\n",
    "max_len = 200\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(oov_token=\"<OOV>\")\n",
    "all_texts = pd.concat([train_df[\"clean_text\"], valid_df[\"clean_text\"], test_df[\"clean_text\"]])\n",
    "tokenizer.fit_on_texts(all_texts)\n",
    "\n",
    "X_train = tokenizer.texts_to_sequences(train_df[\"clean_text\"])\n",
    "X_valid = tokenizer.texts_to_sequences(valid_df[\"clean_text\"])\n",
    "X_test  = tokenizer.texts_to_sequences(test_df[\"clean_text\"])\n",
    "\n",
    "X_train = keras.preprocessing.sequence.pad_sequences(X_train, maxlen=max_len)\n",
    "X_valid = keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=max_len)\n",
    "X_test  = keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_len)\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_valid = valid_df[\"label\"].values\n",
    "y_test  = test_df[\"label\"].values\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(\"Vocabulary size:\", vocab_size)\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ Load GloVe embeddings\n",
    "# ==========================\n",
    "embedding_dim = 100\n",
    "embedding_index = {}\n",
    "\n",
    "# Download GloVe 100d embeddings manually or via web\n",
    "# File: \"glove.6B.100d.txt\"\n",
    "with open(\"glove.6B.100d.txt\", encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vec = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vec\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    vec = embedding_index.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i] = vec\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Define attention layer\n",
    "# ==========================\n",
    "class Attention(layers.Layer):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=\"zeros\",\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"glorot_uniform\",\n",
    "                                 trainable=True)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Alignment scores\n",
    "        u_it = tf.tanh(tf.tensordot(inputs, self.W, axes=1) + self.b)\n",
    "        scores = tf.tensordot(u_it, self.u, axes=1)\n",
    "        alphas = tf.nn.softmax(scores, axis=1)\n",
    "        output = tf.reduce_sum(inputs * alphas, axis=1)\n",
    "        return output\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Build model with BiLSTM + Attention\n",
    "# ==========================\n",
    "inputs = keras.Input(shape=(max_len,))\n",
    "x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
    "                     weights=[embedding_matrix], trainable=False)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2))(x)\n",
    "x = Attention()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Train model\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=5,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# üîü Evaluate model\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# 11Ô∏è‚É£ Predict new text\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    cleaned = spacy_clean_text(text)\n",
    "    seq = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = keras.preprocessing.sequence.pad_sequences(seq, maxlen=max_len)\n",
    "    pred = model.predict(padded)[0][0]\n",
    "    sentiment = \"üòä Positive\" if pred > 0.5 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred:.3f} ‚Üí {sentiment}\")\n",
    "\n",
    "# Example\n",
    "predict_sentiment(\"I really loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"Terrible film, I wasted my time.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key improvements in this version:\n",
    "\n",
    "1. **Global vocabulary** ‚Äì ensures all words from train/valid/test are included.\n",
    "2. **Pre-trained GloVe embeddings** ‚Äì semantic meaning of words is captured.\n",
    "3. **Bidirectional LSTM** ‚Äì context from both directions.\n",
    "4. **Attention layer** ‚Äì focuses on important words in the sentence.\n",
    "5. **Trainable Dense layer** ‚Äì combines features before final sigmoid output.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, the **next step could be visualizing attention weights** to see which words the model focuses on for predictions.\n",
    "\n",
    "Do you want me to add that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad2c7f46",
   "metadata": {},
   "source": [
    "Got it! If you want a **completely new approach**, the most powerful method today for sentiment analysis is to **fine-tune a pretrained transformer model** (like BERT, RoBERTa, or DistilBERT) directly on your dataset. This is much more effective than training from scratch or using static embeddings like GloVe.\n",
    "\n",
    "Below is a **full end-to-end code using Hugging Face Transformers with TensorFlow** to fine-tune a pretrained model on your dataset.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Install libraries\n",
    "# ==========================\n",
    "# !pip install transformers datasets tensorflow\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Import libraries\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Valid:\", valid_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Pretrained model & tokenizer\n",
    "# ==========================\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"  # lightweight, fast\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Encode datasets\n",
    "# ==========================\n",
    "MAX_LEN = 200\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(train_df[\"text\"])\n",
    "valid_enc = encode_texts(valid_df[\"text\"])\n",
    "test_enc  = encode_texts(test_df[\"text\"])\n",
    "\n",
    "y_train = tf.convert_to_tensor(train_df[\"label\"].values)\n",
    "y_valid = tf.convert_to_tensor(valid_df[\"label\"].values)\n",
    "y_test  = tf.convert_to_tensor(test_df[\"label\"].values)\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ Build TF dataset\n",
    "# ==========================\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_enc),\n",
    "    y_train\n",
    ")).shuffle(20000).batch(16)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(valid_enc),\n",
    "    y_valid\n",
    ")).batch(16)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_enc),\n",
    "    y_test\n",
    ")).batch(16)\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Load pretrained model for classification\n",
    "# ==========================\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels=2  # binary classification\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Compile model\n",
    "# ==========================\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Train model (fine-tuning)\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=3\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# üîü Evaluate model\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# 11Ô∏è‚É£ Prediction function\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    enc = tokenizer(text, max_length=MAX_LEN, padding=\"max_length\",\n",
    "                    truncation=True, return_tensors=\"tf\")\n",
    "    logits = model(enc).logits\n",
    "    pred = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    sentiment = \"üòä Positive\" if pred == 1 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred} ‚Üí {sentiment}\")\n",
    "\n",
    "# Examples\n",
    "predict_sentiment(\"I really loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"Terrible film, I wasted my time.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Advantages of This Approach\n",
    "\n",
    "1. **Fine-tunes a pretrained transformer** ‚Äì leverages massive pretraining on language data.\n",
    "2. **Better handling of context and negation** ‚Äì unlike LSTM or static embeddings.\n",
    "3. **Simple code with Hugging Face** ‚Äì tokenizer + model handles everything.\n",
    "4. **Easy to scale** ‚Äì you can switch to larger models (BERT, RoBERTa, DeBERTa) for higher accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can write an **even more optimized version with early stopping, learning rate scheduler, and mixed precision** to train faster and get better performance on your dataset.\n",
    "\n",
    "Do you want me to do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4234bb0a",
   "metadata": {},
   "source": [
    "Perfect! Let‚Äôs upgrade the previous **transformer fine-tuning code** with **performance optimizations**:\n",
    "\n",
    "* **Early stopping** to prevent overfitting.\n",
    "* **Learning rate scheduler** (reduce LR on plateau).\n",
    "* **Mixed precision training** for faster GPU usage.\n",
    "* **Smarter batching**.\n",
    "\n",
    "Here‚Äôs the **full optimized code**:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Install libraries\n",
    "# ==========================\n",
    "# !pip install transformers datasets tensorflow\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Import libraries\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Valid:\", valid_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Enable mixed precision\n",
    "# ==========================\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Pretrained model & tokenizer\n",
    "# ==========================\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"  # lightweight, fast\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "MAX_LEN = 200\n",
    "\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(train_df[\"text\"])\n",
    "valid_enc = encode_texts(valid_df[\"text\"])\n",
    "test_enc  = encode_texts(test_df[\"text\"])\n",
    "\n",
    "y_train = tf.convert_to_tensor(train_df[\"label\"].values)\n",
    "y_valid = tf.convert_to_tensor(valid_df[\"label\"].values)\n",
    "y_test  = tf.convert_to_tensor(test_df[\"label\"].values)\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ Build TF datasets with batching & prefetch\n",
    "# ==========================\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_enc),\n",
    "    y_train\n",
    ")).shuffle(20000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(valid_enc),\n",
    "    y_valid\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(test_enc),\n",
    "    y_test\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Load pretrained model for classification\n",
    "# ==========================\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Compile model with optimizer, loss, metrics\n",
    "# ==========================\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Callbacks: EarlyStopping + ReduceLROnPlateau\n",
    "# ==========================\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-6)\n",
    "\n",
    "# ==========================\n",
    "# üîü Train model (fine-tuning)\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 11Ô∏è‚É£ Evaluate model\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# 12Ô∏è‚É£ Prediction function\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    enc = tokenizer(text, max_length=MAX_LEN, padding=\"max_length\",\n",
    "                    truncation=True, return_tensors=\"tf\")\n",
    "    logits = model(enc).logits\n",
    "    pred = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    sentiment = \"üòä Positive\" if pred == 1 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred} ‚Üí {sentiment}\")\n",
    "\n",
    "# Examples\n",
    "predict_sentiment(\"I really loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"Terrible film, I wasted my time.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Optimizations\n",
    "\n",
    "1. **Mixed precision**: reduces memory usage and speeds up training on GPUs.\n",
    "2. **Early stopping**: stops training when validation loss stops improving.\n",
    "3. **ReduceLROnPlateau**: reduces learning rate if model plateaus for faster convergence.\n",
    "4. **Prefetching**: improves input pipeline speed.\n",
    "5. **Fine-tuning a transformer**: leverages language understanding for better sentiment prediction.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can **also add visualizations for model performance and attention-like heatmaps** for which words influenced the sentiment. This is really cool for interpretability.\n",
    "\n",
    "Do you want me to add that next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf23277",
   "metadata": {},
   "source": [
    "Perfect! Since you want **Transformers with pretrained embeddings and a global vocabulary**, the best approach is to **fine-tune a pretrained transformer (like BERT, DistilBERT, RoBERTa, or ALBERT)** while keeping the tokenizer‚Äôs global vocabulary. This combines:\n",
    "\n",
    "* **Pretrained embeddings** from the model (capturing semantic meaning).\n",
    "* **Global vocab** (shared across your dataset).\n",
    "* **Full fine-tuning** on your sentiment dataset.\n",
    "\n",
    "Below is a **complete, clean, end-to-end TensorFlow script** for your dataset:\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Install libraries\n",
    "# ==========================\n",
    "# !pip install transformers datasets tensorflow\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Imports\n",
    "# ==========================\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "print(\"Train:\", train_df.shape, \"Valid:\", valid_df.shape, \"Test:\", test_df.shape)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Enable mixed precision (optional, speeds up training on GPU)\n",
    "# ==========================\n",
    "tf.keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Pretrained transformer & tokenizer\n",
    "# ==========================\n",
    "PRETRAINED_MODEL = \"distilbert-base-uncased\"  # Fast, small, pretrained embeddings\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_MODEL)\n",
    "MAX_LEN = 200  # max token length\n",
    "\n",
    "# Encode text\n",
    "def encode_texts(texts):\n",
    "    return tokenizer(\n",
    "        list(texts),\n",
    "        max_length=MAX_LEN,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "train_enc = encode_texts(train_df[\"text\"])\n",
    "valid_enc = encode_texts(valid_df[\"text\"])\n",
    "test_enc  = encode_texts(test_df[\"text\"])\n",
    "\n",
    "y_train = tf.convert_to_tensor(train_df[\"label\"].values)\n",
    "y_valid = tf.convert_to_tensor(valid_df[\"label\"].values)\n",
    "y_test  = tf.convert_to_tensor(test_df[\"label\"].values)\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ Build TF datasets\n",
    "# ==========================\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((dict(train_enc), y_train))\\\n",
    "                .shuffle(20000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices((dict(valid_enc), y_valid))\\\n",
    "                .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((dict(test_enc), y_test))\\\n",
    "                .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Load pretrained model\n",
    "# ==========================\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL,\n",
    "    num_labels=2  # binary sentiment\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Compile model\n",
    "# ==========================\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy(\"accuracy\")\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Callbacks\n",
    "# ==========================\n",
    "early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, min_lr=1e-6)\n",
    "\n",
    "# ==========================\n",
    "# üîü Train model (fine-tuning)\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=5,\n",
    "    callbacks=[early_stop, reduce_lr]\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 11Ô∏è‚É£ Evaluate model\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(test_dataset)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# 12Ô∏è‚É£ Prediction function\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    enc = tokenizer(text, max_length=MAX_LEN, padding=\"max_length\",\n",
    "                    truncation=True, return_tensors=\"tf\")\n",
    "    logits = model(enc).logits\n",
    "    pred = tf.argmax(logits, axis=1).numpy()[0]\n",
    "    sentiment = \"üòä Positive\" if pred == 1 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred} ‚Üí {sentiment}\")\n",
    "\n",
    "# Examples\n",
    "predict_sentiment(\"I really loved this movie, it was fantastic!\")\n",
    "predict_sentiment(\"Terrible film, I wasted my time.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Highlights\n",
    "\n",
    "1. **Global vocab**: Using the tokenizer from `distilbert-base-uncased`, ensures the vocabulary is consistent and pretrained embeddings are used.\n",
    "2. **Pretrained embeddings**: Each token uses embeddings learned on huge corpora.\n",
    "3. **Fine-tuning**: The model adjusts embeddings and attention weights to your dataset.\n",
    "4. **Mixed precision & callbacks**: Faster training, early stopping prevents overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can also **extend this code to combine global vocabulary with a small custom embedding layer** so words outside the pretrained vocab still get meaningful embeddings. This can improve performance for domain-specific terms.\n",
    "\n",
    "Do you want me to do that next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea302685",
   "metadata": {},
   "source": [
    "Ah! Now I understand ‚Äî you want to **build a transformer from scratch**, without relying on pretrained models. This means you will implement **your own transformer encoder** for sentiment analysis with **trainable embeddings** and **global vocabulary**.\n",
    "\n",
    "We can do this fully in TensorFlow/Keras. Here‚Äôs a **step-by-step, complete code**:\n",
    "\n",
    "---\n",
    "\n",
    "### **Full Transformer from Scratch for Sentiment Analysis**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import LayerNormalization, Dense, Embedding, Dropout\n",
    "from tensorflow.keras.layers import Layer, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ==========================\n",
    "# 1Ô∏è‚É£ Load dataset\n",
    "# ==========================\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "valid_df = pd.read_csv(\"data/valid.csv\")\n",
    "test_df  = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "texts = pd.concat([train_df['text'], valid_df['text'], test_df['text']])\n",
    "labels = pd.concat([train_df['label'], valid_df['label'], test_df['label']])\n",
    "\n",
    "# ==========================\n",
    "# 2Ô∏è‚É£ Tokenization\n",
    "# ==========================\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "VOCAB_SIZE = 20000\n",
    "MAX_LEN = 200\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "X_train = pad_sequences(tokenizer.texts_to_sequences(train_df['text']), maxlen=MAX_LEN)\n",
    "X_valid = pad_sequences(tokenizer.texts_to_sequences(valid_df['text']), maxlen=MAX_LEN)\n",
    "X_test  = pad_sequences(tokenizer.texts_to_sequences(test_df['text']), maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df['label'].values\n",
    "y_valid = valid_df['label'].values\n",
    "y_test  = test_df['label'].values\n",
    "\n",
    "# ==========================\n",
    "# 3Ô∏è‚É£ Positional Encoding\n",
    "# ==========================\n",
    "def positional_encoding(max_len, d_model):\n",
    "    pos = np.arange(max_len)[:, np.newaxis]\n",
    "    i = np.arange(d_model)[np.newaxis, :]\n",
    "    angle_rates = 1 / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "    angle_rads = pos * angle_rates\n",
    "    pos_encoding = np.zeros((max_len, d_model))\n",
    "    pos_encoding[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "    pos_encoding[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    return tf.cast(pos_encoding[np.newaxis, ...], dtype=tf.float32)\n",
    "\n",
    "# ==========================\n",
    "# 4Ô∏è‚É£ Multi-Head Attention\n",
    "# ==========================\n",
    "class MultiHeadSelfAttention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        self.num_heads = num_heads\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = Dense(d_model)\n",
    "        self.wk = Dense(d_model)\n",
    "        self.wv = Dense(d_model)\n",
    "        self.dense = Dense(d_model)\n",
    "    \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0,2,1,3])\n",
    "    \n",
    "    def call(self, v, k, q, mask=None):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        q = self.split_heads(self.wq(q), batch_size)\n",
    "        k = self.split_heads(self.wk(k), batch_size)\n",
    "        v = self.split_heads(self.wv(v), batch_size)\n",
    "        \n",
    "        scores = tf.matmul(q, k, transpose_b=True) / tf.math.sqrt(tf.cast(self.depth, tf.float32))\n",
    "        if mask is not None:\n",
    "            scores += (mask * -1e9)\n",
    "        weights = tf.nn.softmax(scores, axis=-1)\n",
    "        output = tf.matmul(weights, v)\n",
    "        output = tf.transpose(output, perm=[0,2,1,3])\n",
    "        output = tf.reshape(output, (batch_size, -1, self.num_heads*self.depth))\n",
    "        return self.dense(output)\n",
    "\n",
    "# ==========================\n",
    "# 5Ô∏è‚É£ Transformer Encoder Block\n",
    "# ==========================\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = MultiHeadSelfAttention(d_model, num_heads)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            Dense(dff, activation='relu'),\n",
    "            Dense(d_model)\n",
    "        ])\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask=None):\n",
    "        attn_output = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "# ==========================\n",
    "# 6Ô∏è‚É£ Build the Transformer model\n",
    "# ==========================\n",
    "def build_transformer(vocab_size, max_len, d_model=128, num_heads=4, dff=512, num_layers=2):\n",
    "    inputs = Input(shape=(max_len,))\n",
    "    x = Embedding(vocab_size, d_model)(inputs)\n",
    "    x += positional_encoding(max_len, d_model)\n",
    "    \n",
    "    for _ in range(num_layers):\n",
    "        x = TransformerEncoder(d_model, num_heads, dff)(x, training=True)\n",
    "    \n",
    "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    outputs = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "model = build_transformer(VOCAB_SIZE, MAX_LEN)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# ==========================\n",
    "# 7Ô∏è‚É£ Train\n",
    "# ==========================\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_valid, y_valid),\n",
    "    epochs=5,\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "# ==========================\n",
    "# 8Ô∏è‚É£ Evaluate\n",
    "# ==========================\n",
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {acc:.3f}\")\n",
    "\n",
    "# ==========================\n",
    "# 9Ô∏è‚É£ Predict\n",
    "# ==========================\n",
    "def predict_sentiment(text):\n",
    "    seq = pad_sequences(tokenizer.texts_to_sequences([text]), maxlen=MAX_LEN)\n",
    "    pred = model.predict(seq)[0][0]\n",
    "    sentiment = \"üòä Positive\" if pred >= 0.5 else \"‚òπÔ∏è Negative\"\n",
    "    print(f\"Prediction: {pred:.3f} ‚Üí {sentiment}\")\n",
    "\n",
    "# Examples\n",
    "predict_sentiment(\"I loved this movie, amazing experience!\")\n",
    "predict_sentiment(\"It was boring and too long.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Key Features\n",
    "\n",
    "1. **Trainable Embeddings**: Words get embeddings optimized for your dataset.\n",
    "2. **Global vocabulary**: Defined with `Tokenizer(num_words=VOCAB_SIZE)`.\n",
    "3. **Custom Transformer Encoder**: Multi-head attention + feed-forward + residuals + layer norm.\n",
    "4. **Stackable layers**: Easily increase `num_layers` or `num_heads` for more capacity.\n",
    "5. **GlobalAveragePooling**: Reduces sequence to fixed-size vector for classification.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
