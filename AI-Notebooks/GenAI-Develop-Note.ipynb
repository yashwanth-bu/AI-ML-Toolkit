{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b54208b",
   "metadata": {},
   "source": [
    "## <b>Venilla GAN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce22d655",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64  # smaller for fast GAN training\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 100\n",
    "EPOCHS = 2000\n",
    "SAVE_INTERVAL = 200\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./127.5 - 1)  # scale to [-1,1]\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(train_dir),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Build Generator\n",
    "# -------------------------------\n",
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8*8*128, input_dim=LATENT_DIM))\n",
    "    model.add(layers.Reshape((8, 8, 128)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.Conv2DTranspose(3, kernel_size=4, strides=2, padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Build Discriminator\n",
    "# -------------------------------\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, kernel_size=4, strides=2, padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Conv2D(128, kernel_size=4, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU(0.2))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# GAN Setup\n",
    "# -------------------------------\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "generator = build_generator()\n",
    "\n",
    "z = layers.Input(shape=(LATENT_DIM,))\n",
    "img = generator(z)\n",
    "discriminator.trainable = False\n",
    "valid = discriminator(img)\n",
    "\n",
    "gan = models.Model(z, valid)\n",
    "gan.compile(optimizer='adam', loss='binary_crossentropy')\n",
    "\n",
    "# -------------------------------\n",
    "# Training Loop\n",
    "# -------------------------------\n",
    "def save_images(epoch, generator, examples=4):\n",
    "    noise = np.random.normal(0, 1, (examples, LATENT_DIM))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    gen_imgs = 0.5 * gen_imgs + 0.5  # scale back to [0,1]\n",
    "    \n",
    "    plt.figure(figsize=(4,4))\n",
    "    for i in range(examples):\n",
    "        plt.subplot(2,2,i+1)\n",
    "        plt.imshow(gen_imgs[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # ---------------------\n",
    "    # Train Discriminator\n",
    "    # ---------------------\n",
    "    imgs = dog_gen.next()\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, LATENT_DIM))\n",
    "    gen_imgs = generator.predict(noise)\n",
    "    \n",
    "    real_y = np.ones((BATCH_SIZE, 1))\n",
    "    fake_y = np.zeros((BATCH_SIZE, 1))\n",
    "    \n",
    "    d_loss_real = discriminator.train_on_batch(imgs, real_y)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_imgs, fake_y)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # ---------------------\n",
    "    # Train Generator\n",
    "    # ---------------------\n",
    "    noise = np.random.normal(0, 1, (BATCH_SIZE, LATENT_DIM))\n",
    "    valid_y = np.ones((BATCH_SIZE, 1))\n",
    "    g_loss = gan.train_on_batch(noise, valid_y)\n",
    "    \n",
    "    # ---------------------\n",
    "    # Print & Save\n",
    "    # ---------------------\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]:.4f}] [G loss: {g_loss:.4f}]\")\n",
    "    if epoch % SAVE_INTERVAL == 0:\n",
    "        save_images(epoch, generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fab41dd",
   "metadata": {},
   "source": [
    "## <b>CNN Autoencoder (AE)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b805285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(train_dir),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Autoencoder Model\n",
    "# -------------------------------\n",
    "input_img = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "\n",
    "# Encoder\n",
    "x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "x = layers.Conv2D(256, (3,3), activation='relu', padding='same')(x)\n",
    "encoded = layers.MaxPooling2D((2,2), padding='same')(x)\n",
    "\n",
    "# Decoder\n",
    "x = layers.Conv2D(256, (3,3), activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2,2))(x)\n",
    "decoded = layers.Conv2D(3, (3,3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = models.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "autoencoder.fit(dog_gen, epochs=EPOCHS)\n",
    "\n",
    "# -------------------------------\n",
    "# Generate & Show Reconstructed Images\n",
    "# -------------------------------\n",
    "def show_reconstructed(gen, n=5):\n",
    "    imgs = gen.next()\n",
    "    reconstructed = autoencoder.predict(imgs)\n",
    "    \n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i in range(n):\n",
    "        # Original\n",
    "        plt.subplot(2,n,i+1)\n",
    "        plt.imshow(imgs[i])\n",
    "        plt.axis('off')\n",
    "        # Reconstructed\n",
    "        plt.subplot(2,n,i+1+n)\n",
    "        plt.imshow(reconstructed[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "show_reconstructed(dog_gen)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60a8a8",
   "metadata": {},
   "source": [
    "## <b>Variational Autoencoder (VAE)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e92981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64  # smaller for faster training\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIM = 128\n",
    "EPOCHS = 50\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(train_dir),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Sampling Layer\n",
    "# -------------------------------\n",
    "class Sampling(layers.Layer):\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = tf.random.normal(shape=tf.shape(z_mean))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "# -------------------------------\n",
    "# Encoder\n",
    "# -------------------------------\n",
    "encoder_inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = layers.Conv2D(32, 3, activation='relu', padding='same')(encoder_inputs)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "z_mean = layers.Dense(LATENT_DIM)(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM)(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "encoder = models.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Decoder\n",
    "# -------------------------------\n",
    "latent_inputs = layers.Input(shape=(LATENT_DIM,))\n",
    "x = layers.Dense(8*8*128, activation='relu')(latent_inputs)\n",
    "x = layers.Reshape((8,8,128))(x)\n",
    "x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)\n",
    "decoder_outputs = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)\n",
    "decoder = models.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# VAE Model\n",
    "# -------------------------------\n",
    "class VAE(models.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def compile(self, optimizer):\n",
    "        super(VAE, self).compile()\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            data = data[0]\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            # Reconstruction loss\n",
    "            recon_loss = self.loss_fn(data, reconstruction)\n",
    "            # KL divergence\n",
    "            kl_loss = -0.5 * tf.reduce_mean(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            total_loss = recon_loss + kl_loss\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        return {\"loss\": total_loss, \"reconstruction_loss\": recon_loss, \"kl_loss\": kl_loss}\n",
    "\n",
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "\n",
    "# -------------------------------\n",
    "# Train VAE\n",
    "# -------------------------------\n",
    "vae.fit(dog_gen, epochs=EPOCHS)\n",
    "\n",
    "# -------------------------------\n",
    "# Generate New Dog Images\n",
    "# -------------------------------\n",
    "def generate_dogs(n=5):\n",
    "    noise = np.random.normal(0,1,(n,LATENT_DIM))\n",
    "    gen_imgs = decoder.predict(noise)\n",
    "    plt.figure(figsize=(10,2))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.imshow(gen_imgs[i])\n",
    "        plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_dogs(n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4223e3b",
   "metadata": {},
   "source": [
    "## <b>Autoregressive (iGPT / Image GPT)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302ba29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 32  # small for autoregressive training\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "SEQ_LEN = IMG_SIZE * IMG_SIZE * CHANNELS  # flatten image to sequence\n",
    "EMBED_DIM = 128\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 4\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(train_dir),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare sequences\n",
    "# -------------------------------\n",
    "def preprocess(imgs):\n",
    "    # Flatten images to sequences\n",
    "    return imgs.reshape((imgs.shape[0], SEQ_LEN))\n",
    "\n",
    "# -------------------------------\n",
    "# Transformer Block\n",
    "# -------------------------------\n",
    "def transformer_block(x, embed_dim, num_heads):\n",
    "    attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x, x)\n",
    "    x = layers.LayerNormalization()(x + attn_output)\n",
    "    ff = layers.Dense(embed_dim*4, activation='relu')(x)\n",
    "    ff = layers.Dense(embed_dim)(ff)\n",
    "    x = layers.LayerNormalization()(x + ff)\n",
    "    return x\n",
    "\n",
    "# -------------------------------\n",
    "# Autoregressive Image GPT Model\n",
    "# -------------------------------\n",
    "inputs = layers.Input(shape=(SEQ_LEN,))\n",
    "x = layers.Embedding(input_dim=256, output_dim=EMBED_DIM)(tf.cast(inputs*255, tf.int32))\n",
    "for _ in range(NUM_LAYERS):\n",
    "    x = transformer_block(x, EMBED_DIM, NUM_HEADS)\n",
    "outputs = layers.Dense(256, activation='softmax')(x)  # predict next pixel intensity\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Custom Data Generator for GPT\n",
    "# -------------------------------\n",
    "class GPTData(tf.keras.utils.Sequence):\n",
    "    def __init__(self, generator):\n",
    "        self.generator = generator\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.generator)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.generator[idx]\n",
    "        seq = preprocess(batch)\n",
    "        x = seq[:, :-1]  # input sequence\n",
    "        y = seq[:, 1:]   # next-pixel prediction\n",
    "        return x, y\n",
    "\n",
    "gpt_gen = GPTData(dog_gen)\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "model.fit(gpt_gen, epochs=EPOCHS)\n",
    "\n",
    "# -------------------------------\n",
    "# Generate new image\n",
    "# -------------------------------\n",
    "def generate_image():\n",
    "    seq = np.zeros((1, SEQ_LEN-1))\n",
    "    for i in range(SEQ_LEN-1):\n",
    "        preds = model.predict(seq, verbose=0)\n",
    "        next_pixel = np.argmax(preds[0, i])\n",
    "        seq[0, i] = next_pixel / 255.0\n",
    "    img = seq.reshape((IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36de7356",
   "metadata": {},
   "source": [
    "## <b>Diffusion Models<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e31dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 32\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "TIMESTEPS = 100  # diffusion steps\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Simple UNet-like model for denoising\n",
    "# -------------------------------\n",
    "def get_unet(img_shape=(IMG_SIZE, IMG_SIZE, CHANNELS)):\n",
    "    inputs = layers.Input(img_shape)\n",
    "    t_input = layers.Input(shape=(1,))  # timestep input\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D()(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    outputs = layers.Conv2D(CHANNELS, 3, activation='sigmoid', padding='same')(x)\n",
    "    return models.Model([inputs, t_input], outputs)\n",
    "\n",
    "model = get_unet()\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Noise schedule\n",
    "# -------------------------------\n",
    "def add_noise(x, t):\n",
    "    noise = tf.random.normal(shape=tf.shape(x))\n",
    "    alpha = 1 - t / TIMESTEPS\n",
    "    return tf.sqrt(alpha) * x + tf.sqrt(1 - alpha) * noise, noise\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dog_gen:\n",
    "        t = np.random.randint(1, TIMESTEPS+1)\n",
    "        noisy_imgs, noise = add_noise(batch, t)\n",
    "        loss = model.train_on_batch([noisy_imgs, np.full((len(batch),1), t)], noise)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {loss}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Sampling / Generation\n",
    "# -------------------------------\n",
    "def generate_image():\n",
    "    img = tf.random.normal((1, IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    for t in reversed(range(1, TIMESTEPS+1)):\n",
    "        pred_noise = model.predict([img, np.array([[t]])])\n",
    "        alpha = 1 - t / TIMESTEPS\n",
    "        img = (img - tf.sqrt(1-alpha) * pred_noise) / tf.sqrt(alpha)\n",
    "    plt.imshow(np.clip(img[0], 0, 1))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc26f7e",
   "metadata": {},
   "source": [
    "## <b>Latent Diffusion Models (LDMs)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 32\n",
    "LATENT_DIM = 16  # small latent dimension\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "TIMESTEPS = 50\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Encoder: Pixel -> Latent\n",
    "# -------------------------------\n",
    "def build_encoder():\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    latent = layers.Dense(LATENT_DIM)(x)\n",
    "    return models.Model(inputs, latent, name='encoder')\n",
    "\n",
    "# -------------------------------\n",
    "# Decoder: Latent -> Pixel\n",
    "# -------------------------------\n",
    "def build_decoder():\n",
    "    inputs = layers.Input(shape=(LATENT_DIM,))\n",
    "    x = layers.Dense(8*8*128, activation='relu')(inputs)\n",
    "    x = layers.Reshape((8,8,128))(x)\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)\n",
    "    outputs = layers.Conv2D(CHANNELS, 3, activation='sigmoid', padding='same')(x)\n",
    "    return models.Model(inputs, outputs, name='decoder')\n",
    "\n",
    "encoder = build_encoder()\n",
    "decoder = build_decoder()\n",
    "\n",
    "# -------------------------------\n",
    "# UNet-like Denoiser in latent space\n",
    "# -------------------------------\n",
    "def get_latent_unet(latent_dim):\n",
    "    latent_input = layers.Input(shape=(latent_dim,))\n",
    "    t_input = layers.Input(shape=(1,))\n",
    "    x = layers.Dense(128, activation='relu')(latent_input)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    x = layers.Dense(latent_dim)(x)  # predict noise\n",
    "    return models.Model([latent_input, t_input], x, name='latent_unet')\n",
    "\n",
    "latent_unet = get_latent_unet(LATENT_DIM)\n",
    "latent_unet.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# -------------------------------\n",
    "# Diffusion helper\n",
    "# -------------------------------\n",
    "def add_noise(latent, t):\n",
    "    noise = tf.random.normal(shape=tf.shape(latent))\n",
    "    alpha = 1 - t / TIMESTEPS\n",
    "    return tf.sqrt(alpha) * latent + tf.sqrt(1-alpha) * noise, noise\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dog_gen:\n",
    "        latent = encoder.predict(batch)\n",
    "        t = np.random.randint(1, TIMESTEPS+1)\n",
    "        noisy_latent, noise = add_noise(latent, t)\n",
    "        loss = latent_unet.train_on_batch([noisy_latent, np.full((len(batch),1), t)], noise)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} - Loss: {loss}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Generate new dog image\n",
    "# -------------------------------\n",
    "def generate_image():\n",
    "    latent = tf.random.normal((1, LATENT_DIM))\n",
    "    for t in reversed(range(1, TIMESTEPS+1)):\n",
    "        pred_noise = latent_unet.predict([latent, np.array([[t]])])\n",
    "        alpha = 1 - t / TIMESTEPS\n",
    "        latent = (latent - tf.sqrt(1-alpha) * pred_noise) / tf.sqrt(alpha)\n",
    "    img = decoder.predict(latent)\n",
    "    plt.imshow(np.clip(img[0],0,1))\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "generate_image()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16518e87",
   "metadata": {},
   "source": [
    "## <b>GAN (DCGAN / cGAN / StyleGAN / CycleGAN)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb98f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000\n",
    "NOISE_DIM = 100\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./127.5 - 1)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Generator\n",
    "# -------------------------------\n",
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((8,8,256)))\n",
    "    model.add(layers.Conv2DTranspose(128, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(64, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(CHANNELS, 5, strides=2, padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Discriminator\n",
    "# -------------------------------\n",
    "def build_discriminator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(64, 5, strides=2, padding='same', input_shape=(IMG_SIZE,IMG_SIZE,CHANNELS)))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Conv2D(128, 5, strides=2, padding='same'))\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Dropout(0.3))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(1))\n",
    "    return model\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizers and loss\n",
    "# -------------------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -------------------------------\n",
    "# Training step\n",
    "# -------------------------------\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = (cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                     cross_entropy(tf.zeros_like(fake_output), fake_output)) * 0.5\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dog_gen:\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "        break  # only one batch per step for simplicity\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
    "        # Generate sample\n",
    "        noise = tf.random.normal([1, NOISE_DIM])\n",
    "        img = generator(noise, training=False)\n",
    "        plt.imshow((img[0]+1)/2)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d217303",
   "metadata": {},
   "source": [
    "## <b>Text-to-Image cGAN (from scratch)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd4769",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000\n",
    "NOISE_DIM = 100\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./127.5 - 1)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Generator (CNN-based)\n",
    "# -------------------------------\n",
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((8,8,256)))\n",
    "    model.add(layers.Conv2DTranspose(128, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(64, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(CHANNELS, 5, strides=2, padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Vision Transformer (ViT) Discriminator\n",
    "# -------------------------------\n",
    "def build_vit_discriminator(img_size=IMG_SIZE, channels=CHANNELS, patch_size=8, embed_dim=64, num_heads=4, mlp_dim=128):\n",
    "    input_img = layers.Input(shape=(img_size,img_size,channels))\n",
    "    # Create patches\n",
    "    patches = layers.Conv2D(embed_dim, patch_size, strides=patch_size, padding='valid')(input_img)\n",
    "    # Flatten patches\n",
    "    patches = layers.Reshape((-1, embed_dim))(patches)\n",
    "    # Add positional embeddings\n",
    "    num_patches = patches.shape[1]\n",
    "    pos_emb = layers.Embedding(input_dim=num_patches, output_dim=embed_dim)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    x = patches + pos_emb(positions)\n",
    "    # Transformer encoder\n",
    "    for _ in range(2):\n",
    "        # Layer norm\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        # Multi-head attention\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)(x1,x1)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        # MLP\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x1 = layers.Dense(mlp_dim, activation='relu')(x1)\n",
    "        x1 = layers.Dense(embed_dim)(x1)\n",
    "        x = layers.Add()([x, x1])\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "    return models.Model(input_img, out)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_vit_discriminator()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizers and loss\n",
    "# -------------------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -------------------------------\n",
    "# Training step\n",
    "# -------------------------------\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = (cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                     cross_entropy(tf.zeros_like(fake_output), fake_output)) * 0.5\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dog_gen:\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "        break  # one batch per step for simplicity\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
    "        # Generate sample\n",
    "        noise = tf.random.normal([1, NOISE_DIM])\n",
    "        img = generator(noise, training=False)\n",
    "        plt.imshow((img[0]+1)/2)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d4abd",
   "metadata": {},
   "source": [
    "## <b>CNN + VIT<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394d8b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 2000\n",
    "NOISE_DIM = 100\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./127.5 - 1)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Generator (CNN-based)\n",
    "# -------------------------------\n",
    "def build_generator():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Dense(8*8*256, use_bias=False, input_shape=(NOISE_DIM,)))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.LeakyReLU())\n",
    "    model.add(layers.Reshape((8,8,256)))\n",
    "    model.add(layers.Conv2DTranspose(128, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(64, 5, strides=2, padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2DTranspose(CHANNELS, 5, strides=2, padding='same', activation='tanh'))\n",
    "    return model\n",
    "\n",
    "# -------------------------------\n",
    "# Discriminator (CNN + ViT hybrid)\n",
    "# -------------------------------\n",
    "def build_discriminator(img_size=IMG_SIZE, channels=CHANNELS, patch_size=8, embed_dim=64, num_heads=4, mlp_dim=128):\n",
    "    input_img = layers.Input(shape=(img_size,img_size,channels))\n",
    "    \n",
    "    # CNN feature extractor\n",
    "    x = layers.Conv2D(64, 5, strides=2, padding='same', activation='relu')(input_img)\n",
    "    x = layers.Conv2D(128, 5, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2D(embed_dim, 3, strides=1, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Flatten patches for ViT\n",
    "    num_patches = (img_size // patch_size) ** 2\n",
    "    patch_dim = embed_dim * patch_size * patch_size\n",
    "    x = layers.Reshape((num_patches, patch_dim))(x)\n",
    "    \n",
    "    # Positional embedding\n",
    "    pos_emb = layers.Embedding(input_dim=num_patches, output_dim=patch_dim)\n",
    "    positions = tf.range(start=0, limit=num_patches, delta=1)\n",
    "    x = x + pos_emb(positions)\n",
    "    \n",
    "    # Transformer encoder\n",
    "    for _ in range(2):\n",
    "        # Layer norm\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        # Multi-head attention\n",
    "        attn_output = layers.MultiHeadAttention(num_heads=num_heads, key_dim=patch_dim)(x1, x1)\n",
    "        x = layers.Add()([x, attn_output])\n",
    "        # MLP\n",
    "        x1 = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        x1 = layers.Dense(mlp_dim, activation='relu')(x1)\n",
    "        x1 = layers.Dense(patch_dim)(x1)\n",
    "        x = layers.Add()([x, x1])\n",
    "    \n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    out = layers.Dense(1)(x)\n",
    "    \n",
    "    return models.Model(input_img, out)\n",
    "\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizers and loss\n",
    "# -------------------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -------------------------------\n",
    "# Training step\n",
    "# -------------------------------\n",
    "@tf.function\n",
    "def train_step(images):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator(noise, training=True)\n",
    "\n",
    "        real_output = discriminator(images, training=True)\n",
    "        fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = (cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                     cross_entropy(tf.zeros_like(fake_output), fake_output)) * 0.5\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dog_gen:\n",
    "        gen_loss, disc_loss = train_step(batch)\n",
    "        break  # one batch per step for simplicity\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
    "        # Generate sample\n",
    "        noise = tf.random.normal([1, NOISE_DIM])\n",
    "        img = generator(noise, training=False)\n",
    "        plt.imshow((img[0]+1)/2)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40259cb9",
   "metadata": {},
   "source": [
    "## <b>Fine tuning pretrained model<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a51002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies first (run once)\n",
    "# pip install diffusers transformers accelerate torch\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------------------------------\n",
    "# Load pre-trained Stable Diffusion model\n",
    "# -------------------------------\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"  # lightweight, free-to-use version\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\") if torch.cuda.is_available() else pipe.to(\"cpu\")\n",
    "\n",
    "# -------------------------------\n",
    "# Text prompt -> Image\n",
    "# -------------------------------\n",
    "prompt = \"A cute dog playing in the garden, photorealistic\"\n",
    "image = pipe(prompt, guidance_scale=7.5).images[0]\n",
    "\n",
    "# Display the generated image\n",
    "plt.imshow(image)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Save the image\n",
    "image.save(\"generated_dog.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de43562b",
   "metadata": {},
   "source": [
    "## <b>Text-to-Image Fine-Tuned Pretrained (Stable Diffusion)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda3f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 64\n",
    "CHANNELS = 3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 1000\n",
    "NOISE_DIM = 100\n",
    "MAX_SEQ_LEN = 10\n",
    "EMBED_DIM = 50\n",
    "TRAIN_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "\n",
    "# -------------------------------\n",
    "# Load dog images\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./127.5 - 1)\n",
    "dog_gen = datagen.flow_from_directory(\n",
    "    os.path.dirname(TRAIN_DIR),\n",
    "    target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    classes=['dogs'],\n",
    "    class_mode=None,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Text prompts (for small example)\n",
    "# -------------------------------\n",
    "# Normally, you would have one prompt per image\n",
    "texts = [\"dog\"] * 1000  # simple repetitive label\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "text_data = pad_sequences(sequences, maxlen=MAX_SEQ_LEN, padding='post')\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# -------------------------------\n",
    "# Generator (CNN + Text Embedding)\n",
    "# -------------------------------\n",
    "def build_generator():\n",
    "    noise_input = layers.Input(shape=(NOISE_DIM,))\n",
    "    text_input = layers.Input(shape=(MAX_SEQ_LEN,))\n",
    "    \n",
    "    # Text embedding\n",
    "    x_text = layers.Embedding(vocab_size, EMBED_DIM, input_length=MAX_SEQ_LEN)(text_input)\n",
    "    x_text = layers.LSTM(128)(x_text)\n",
    "    \n",
    "    # Combine noise + text\n",
    "    x = layers.Concatenate()([noise_input, x_text])\n",
    "    x = layers.Dense(8*8*256, use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU()(x)\n",
    "    x = layers.Reshape((8,8,256))(x)\n",
    "    x = layers.Conv2DTranspose(128, 5, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(64, 5, strides=2, padding='same', activation='relu')(x)\n",
    "    x = layers.Conv2DTranspose(CHANNELS, 5, strides=2, padding='same', activation='tanh')(x)\n",
    "    \n",
    "    return models.Model([noise_input, text_input], x)\n",
    "\n",
    "# -------------------------------\n",
    "# Discriminator (CNN + conditional)\n",
    "# -------------------------------\n",
    "def build_discriminator():\n",
    "    img_input = layers.Input(shape=(IMG_SIZE, IMG_SIZE, CHANNELS))\n",
    "    text_input = layers.Input(shape=(MAX_SEQ_LEN,))\n",
    "    \n",
    "    # Text embedding\n",
    "    x_text = layers.Embedding(vocab_size, EMBED_DIM, input_length=MAX_SEQ_LEN)(text_input)\n",
    "    x_text = layers.LSTM(128)(x_text)\n",
    "    x_text = layers.RepeatVector(IMG_SIZE*IMG_SIZE)(x_text)\n",
    "    x_text = layers.Reshape((IMG_SIZE, IMG_SIZE, 128))(x_text)\n",
    "    \n",
    "    # CNN on image\n",
    "    x_img = layers.Conv2D(64, 5, strides=2, padding='same', activation='relu')(img_input)\n",
    "    x_img = layers.Conv2D(128, 5, strides=2, padding='same', activation='relu')(x_img)\n",
    "    \n",
    "    # Concatenate image + text features\n",
    "    x = layers.Concatenate()([x_img, x_text])\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    return models.Model([img_input, text_input], x)\n",
    "\n",
    "# -------------------------------\n",
    "# Build models\n",
    "# -------------------------------\n",
    "generator = build_generator()\n",
    "discriminator = build_discriminator()\n",
    "\n",
    "# -------------------------------\n",
    "# Optimizers & loss\n",
    "# -------------------------------\n",
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "gen_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "\n",
    "# -------------------------------\n",
    "# Training step\n",
    "# -------------------------------\n",
    "@tf.function\n",
    "def train_step(images, text_seq):\n",
    "    noise = tf.random.normal([BATCH_SIZE, NOISE_DIM])\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_images = generator([noise, text_seq], training=True)\n",
    "\n",
    "        real_output = discriminator([images, text_seq], training=True)\n",
    "        fake_output = discriminator([generated_images, text_seq], training=True)\n",
    "\n",
    "        gen_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "        disc_loss = (cross_entropy(tf.ones_like(real_output), real_output) +\n",
    "                     cross_entropy(tf.zeros_like(fake_output), fake_output)) * 0.5\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    gen_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    disc_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return gen_loss, disc_loss\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop\n",
    "# -------------------------------\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, batch in enumerate(dog_gen):\n",
    "        text_batch = text_data[:BATCH_SIZE]  # use same text for simplicity\n",
    "        gen_loss, disc_loss = train_step(batch, text_batch)\n",
    "        break  # one batch per step for demo\n",
    "    \n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Gen Loss: {gen_loss:.4f}, Disc Loss: {disc_loss:.4f}\")\n",
    "        # Generate sample image\n",
    "        noise = tf.random.normal([1, NOISE_DIM])\n",
    "        sample_img = generator([noise, text_data[:1]], training=False)\n",
    "        plt.imshow((sample_img[0]+1)/2)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e474c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed:\n",
    "# pip install diffusers transformers accelerate datasets safetensors torch torchvision\n",
    "\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline, StableDiffusionTrainer\n",
    "from diffusers import UNet2DConditionModel, AutoencoderKL, PNDMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "DATA_DIR = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\\dogs\"\n",
    "OUTPUT_DIR = \"./fine_tuned_dogs\"\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 5\n",
    "IMG_SIZE = 256\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "PROMPT = \"A cute dog\"  # text prompt for conditioning\n",
    "\n",
    "# -------------------------------\n",
    "# Custom dataset\n",
    "# -------------------------------\n",
    "class DogDataset(Dataset):\n",
    "    def __init__(self, img_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.images = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.endswith((\".jpg\",\".png\"))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.images[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        return {\"image\": img, \"prompt\": PROMPT}\n",
    "\n",
    "# -------------------------------\n",
    "# Transform & DataLoader\n",
    "# -------------------------------\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5]*3, [0.5]*3)\n",
    "])\n",
    "\n",
    "dataset = DogDataset(DATA_DIR, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# -------------------------------\n",
    "# Load pre-trained Stable Diffusion components\n",
    "# -------------------------------\n",
    "vae = AutoencoderKL.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(DEVICE)\n",
    "unet = UNet2DConditionModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\").to(DEVICE)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(DEVICE)\n",
    "scheduler = PNDMScheduler.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "\n",
    "# -------------------------------\n",
    "# Training loop (fine-tuning UNet)\n",
    "# -------------------------------\n",
    "optimizer = torch.optim.Adam(unet.parameters(), lr=1e-5)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        images = batch[\"image\"].to(DEVICE)\n",
    "        prompts = batch[\"prompt\"]\n",
    "        text_inputs = tokenizer(prompts, padding=\"max_length\", max_length=tokenizer.model_max_length, return_tensors=\"pt\").to(DEVICE)\n",
    "        text_embeddings = text_encoder(**text_inputs).last_hidden_state\n",
    "\n",
    "        # Forward pass through UNet\n",
    "        noise = torch.randn_like(images).to(DEVICE)\n",
    "        noisy_images = images + noise  # simplified for demonstration\n",
    "        noise_pred = unet(noisy_images, text_embeddings).sample\n",
    "        loss = loss_fn(noise_pred, noise)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save fine-tuned model\n",
    "# -------------------------------\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "unet.save_pretrained(os.path.join(OUTPUT_DIR, \"unet\"))\n",
    "vae.save_pretrained(os.path.join(OUTPUT_DIR, \"vae\"))\n",
    "text_encoder.save_pretrained(os.path.join(OUTPUT_DIR, \"text_encoder\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"tokenizer\"))\n",
    "\n",
    "# -------------------------------\n",
    "# Generate a sample image\n",
    "# -------------------------------\n",
    "pipe = StableDiffusionPipeline.from_pretrained(OUTPUT_DIR, torch_dtype=torch.float16).to(DEVICE)\n",
    "image = pipe(\"A cute dog playing\", guidance_scale=7.5).images[0]\n",
    "image.save(\"fine_tuned_dog.png\")\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6baecc5a",
   "metadata": {},
   "source": [
    "| **Model**                                                  | **Architecture / Type**            | **Strengths**                                                             | **Weaknesses**                                                      | **Use Case**                                                 |\n",
    "| ---------------------------------------------------------- | ---------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------------- | ------------------------------------------------------------ |\n",
    "| **CNN Autoencoder (AE)**                                   | CNN encoder + CNN decoder          | Simple, easy to implement, reconstructs images well                       | Limited generative capability, blurry outputs, no text conditioning | Image reconstruction, compression, feature learning          |\n",
    "| **Variational Autoencoder (VAE)**                          | CNN + probabilistic latent space   | Generates new images from latent space, smooth latent interpolation       | Often blurry images, less detailed                                  | Unconditional image generation, latent space exploration     |\n",
    "| **Autoregressive (iGPT / Image GPT)**                      | Transformer on pixels              | Captures global dependencies, generates high-fidelity images              | Very computationally expensive, slow sampling                       | Small-scale generative experiments, pixel-wise modeling      |\n",
    "| **Diffusion Models**                                       | Noise-to-image iterative denoising | High-quality images, stable training                                      | Slow generation, heavy computation                                  | Unconditional / conditional image synthesis                  |\n",
    "| **Latent Diffusion Models (LDMs)**                         | Diffusion in latent space          | Faster and lighter than pixel-space diffusion                             | Requires pre-trained VAE, less direct control                       | High-res generation, text-to-image synthesis                 |\n",
    "| **GAN (DCGAN / cGAN / StyleGAN / CycleGAN)**               | CNN-based adversarial training     | Sharp, realistic images; conditional GANs allow text / class conditioning | Training instability, mode collapse                                 | Unconditional / conditional image generation, style transfer |\n",
    "| **Text-to-Image cGAN (from scratch)**                      | CNN generator + LSTM text encoder  | Lightweight, trainable on small datasets, custom prompts                  | Low-quality images on small datasets, limited text understanding    | Educational, small text-to-image experiments                 |\n",
    "| **Text-to-Image Fine-Tuned Pretrained (Stable Diffusion)** | Pretrained UNet + CLIP + VAE       | High-quality, realistic images, text-conditioned, fast convergence        | Requires GPU, fine-tuning still computationally expensive           | Personalized text-to-image generation (e.g., dog images)     |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
