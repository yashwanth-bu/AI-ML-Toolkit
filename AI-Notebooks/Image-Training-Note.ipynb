{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b82121b8",
   "metadata": {},
   "source": [
    "## <b>Simple CNN<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Paths to your data\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# Image data generators\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,  # split train into train + validation\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Generators\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Build CNN model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(150,150,3)),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Conv2D(128, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # binary classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bc0f4b",
   "metadata": {},
   "source": [
    "## <b>CNN + Transformer Hybrid<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe1225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import os\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentation\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(150, 150),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Transfer Learning: MobileNetV2\n",
    "# -------------------------------\n",
    "base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(150,150,3))\n",
    "base_model.trainable = False  # Freeze pre-trained layers\n",
    "\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# -------------------------------\n",
    "# Train the model\n",
    "# -------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate on test set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Optional: Save model\n",
    "# -------------------------------\n",
    "model.save(\"cats_vs_dogs_mobilenetv2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b74e6de",
   "metadata": {},
   "source": [
    "## <b>Vision Transformer (ViT)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6a87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentation\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),  # ViT usually expects 224x224\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Vision Transformer Model\n",
    "# -------------------------------\n",
    "# Load pre-trained ViT from keras.applications\n",
    "# Note: This requires TensorFlow >= 2.12\n",
    "vit_base = tf.keras.applications.VisionTransformer(\n",
    "    include_top=False,\n",
    "    weights='imagenet21k',  # pre-trained on ImageNet-21k\n",
    "    input_shape=(224,224,3),\n",
    "    include_preprocessing=False\n",
    ")\n",
    "\n",
    "vit_base.trainable = False  # Freeze base for transfer learning\n",
    "\n",
    "# Add custom classification head\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(vit_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vit_base.input, outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Train the model\n",
    "# -------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=15\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate on test set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save model\n",
    "# -------------------------------\n",
    "model.save(\"cats_vs_dogs_vit.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad144bbe",
   "metadata": {},
   "source": [
    "## <b>Self-Supervised Learning (SSL) CNN/ViT<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c22e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentation\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224,224),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Load Pre-trained Vision Transformer\n",
    "# -------------------------------\n",
    "# Requires TensorFlow >= 2.12\n",
    "vit_base = tf.keras.applications.VisionTransformer(\n",
    "    include_top=False,\n",
    "    weights='imagenet21k',\n",
    "    input_shape=(224,224,3)\n",
    ")\n",
    "\n",
    "vit_base.trainable = False  # Freeze base for initial training\n",
    "\n",
    "# -------------------------------\n",
    "# Add Custom Head\n",
    "# -------------------------------\n",
    "x = GlobalAveragePooling2D()(vit_base.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = Model(inputs=vit_base.input, outputs=output)\n",
    "\n",
    "# -------------------------------\n",
    "# Compile and Train Top Layers\n",
    "# -------------------------------\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Training top layers...\")\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=5  # Train only top layers first\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Fine-tune the Base Model\n",
    "# -------------------------------\n",
    "# Unfreeze last few layers of ViT\n",
    "vit_base.trainable = True\n",
    "for layer in vit_base.layers[:-50]:  # Freeze all but last 50 layers\n",
    "    layer.trainable = False\n",
    "\n",
    "# Recompile with lower learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Fine-tuning base model...\")\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=10  # Fine-tune for more epochs\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate on Test Set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save Fine-tuned Model\n",
    "# -------------------------------\n",
    "model.save(\"cats_vs_dogs_vit_finetuned.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e780bb",
   "metadata": {},
   "source": [
    "## <b>Graph-based Methods<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74017eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, \n",
    "    GlobalAveragePooling2D, Reshape, LayerNormalization, MultiHeadAttention, Add\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# -------------------------------\n",
    "# Paths\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data Augmentation\n",
    "# -------------------------------\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128,128),  # Smaller size for CNN+Transformer\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(128,128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(128,128),\n",
    "    batch_size=16,\n",
    "    class_mode='binary',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# CNN + Transformer Hybrid Model\n",
    "# -------------------------------\n",
    "def cnn_transformer(input_shape=(128,128,3), num_heads=4, ff_dim=128, num_classes=1):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # --- CNN feature extractor ---\n",
    "    x = Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2,2))(x)\n",
    "    x = Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = MaxPooling2D((2,2))(x)  # Shape: (16,16,128)\n",
    "    \n",
    "    # --- Prepare for Transformer ---\n",
    "    shape = x.shape\n",
    "    x = Reshape((shape[1]*shape[2], shape[3]))(x)  # Flatten to (tokens, channels)\n",
    "    \n",
    "    # --- Transformer Encoder Block ---\n",
    "    # Multi-head self-attention\n",
    "    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=ff_dim)(x, x)\n",
    "    x = Add()([x, attn_output])\n",
    "    x = LayerNormalization()(x)\n",
    "    \n",
    "    # Feed-forward\n",
    "    ff = Dense(ff_dim, activation='relu')(x)\n",
    "    ff = Dense(shape[3])(ff)\n",
    "    x = Add()([x, ff])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # --- Classification Head ---\n",
    "    x = GlobalAveragePooling2D()(Reshape((shape[1], shape[2], shape[3]))(x))\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    outputs = Dense(num_classes, activation='sigmoid')(x)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model = cnn_transformer()\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Train the model\n",
    "# -------------------------------\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // train_generator.batch_size,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // validation_generator.batch_size,\n",
    "    epochs=20\n",
    ")\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate on Test Set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save Model\n",
    "# -------------------------------\n",
    "model.save(\"cats_vs_dogs_cnn_transformer.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604711e7",
   "metadata": {},
   "source": [
    "## <b>Capsule Networks (CapsNet)<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d158274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "# -------------------------------\n",
    "# Paths & parameters\n",
    "# -------------------------------\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir  = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS_SSL = 10\n",
    "EPOCHS_FINETUNE = 15\n",
    "\n",
    "# -------------------------------\n",
    "# Self-Supervised Data Augmentation\n",
    "# -------------------------------\n",
    "def augment(image):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_brightness(image, 0.3)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_crop(image, size=[IMG_SIZE, IMG_SIZE, 3])\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    return image\n",
    "\n",
    "def preprocess(image_path, label):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    return augment(image), augment(image)\n",
    "\n",
    "# Create dataset for SSL\n",
    "image_paths = []\n",
    "for class_dir in os.listdir(train_dir):\n",
    "    class_path = os.path.join(train_dir, class_dir)\n",
    "    for fname in os.listdir(class_path):\n",
    "        if fname.lower().endswith(('.jpg','.png')):\n",
    "            image_paths.append(os.path.join(class_path, fname))\n",
    "\n",
    "labels = [0]*len(image_paths)\n",
    "dataset = tf.data.Dataset.from_tensor_slices((image_paths, labels))\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.shuffle(500).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# -------------------------------\n",
    "# CNN + Transformer Encoder\n",
    "# -------------------------------\n",
    "def get_encoder():\n",
    "    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    # CNN backbone\n",
    "    x = layers.Conv2D(32, (3,3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Conv2D(64, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)\n",
    "    x = layers.Conv2D(128, (3,3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2,2))(x)  # Shape (16,16,128)\n",
    "    \n",
    "    # Flatten for Transformer\n",
    "    shape = x.shape\n",
    "    x = layers.Reshape((shape[1]*shape[2], shape[3]))(x)\n",
    "    \n",
    "    # Transformer Encoder\n",
    "    attn = layers.MultiHeadAttention(num_heads=4, key_dim=128)(x, x)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    ff = layers.Dense(128, activation='relu')(x)\n",
    "    ff = layers.Dense(shape[3])(ff)\n",
    "    x = layers.Add()([x, ff])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    # Global pooling and projection head for SSL\n",
    "    x = layers.Reshape((shape[1], shape[2], shape[3]))(x)\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(128, activation='relu')(x)\n",
    "    outputs = layers.Dense(64)(x)\n",
    "    \n",
    "    return models.Model(inputs, outputs, name=\"encoder\")\n",
    "\n",
    "encoder = get_encoder()\n",
    "\n",
    "# -------------------------------\n",
    "# Contrastive Loss (NT-Xent)\n",
    "# -------------------------------\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    z_i = tf.math.l2_normalize(z_i, axis=1)\n",
    "    z_j = tf.math.l2_normalize(z_j, axis=1)\n",
    "    return tfa.losses.npairs_loss(tf.zeros_like(z_i[:,0]), z_i, z_j)\n",
    "\n",
    "# -------------------------------\n",
    "# Self-Supervised Pretraining\n",
    "# -------------------------------\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "\n",
    "for epoch in range(EPOCHS_SSL):\n",
    "    for batch in dataset:\n",
    "        x1, x2 = batch\n",
    "        with tf.GradientTape() as tape:\n",
    "            z1 = encoder(x1, training=True)\n",
    "            z2 = encoder(x2, training=True)\n",
    "            loss = nt_xent_loss(z1, z2)\n",
    "        grads = tape.gradient(loss, encoder.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, encoder.trainable_variables))\n",
    "    print(f\"[SSL] Epoch {epoch+1}, Loss: {loss.numpy():.4f}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Fine-tuning on Labeled Data\n",
    "# -------------------------------\n",
    "# Add classification head\n",
    "inputs = encoder.input\n",
    "x = encoder.output\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "ssl_model = models.Model(inputs, outputs)\n",
    "\n",
    "ssl_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Data generators\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE, class_mode='binary', subset='training'\n",
    ")\n",
    "val_gen = datagen.flow_from_directory(\n",
    "    train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE, class_mode='binary', subset='validation'\n",
    ")\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    test_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE, class_mode='binary', shuffle=False\n",
    ")\n",
    "\n",
    "# Fine-tune\n",
    "ssl_model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS_FINETUNE)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate on Test Set\n",
    "# -------------------------------\n",
    "test_loss, test_acc = ssl_model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save model\n",
    "# -------------------------------\n",
    "ssl_model.save(\"cats_dogs_cnn_transformer_ssl.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b02ea49",
   "metadata": {},
   "source": [
    "## <b>NAS / Attention-Augmented CNNs<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b7dd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.data import Graph\n",
    "from spektral.models import GCN\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "K = 5  # Number of neighbors in graph\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Extract CNN Features\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "train_gen = datagen.flow_from_directory(\n",
    "    train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE, class_mode='binary', shuffle=False\n",
    ")\n",
    "\n",
    "cnn = ResNet50(weights='imagenet', include_top=False, pooling='avg', input_shape=(IMG_SIZE,IMG_SIZE,3))\n",
    "\n",
    "features = cnn.predict(train_gen)\n",
    "labels = train_gen.classes\n",
    "\n",
    "print(\"Feature shape:\", features.shape)  # (num_samples, 2048)\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Build Graph (k-NN)\n",
    "# -------------------------------\n",
    "A = kneighbors_graph(features, n_neighbors=K, mode='connectivity', include_self=True)\n",
    "A = A.toarray().astype('float32')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: GCN for Node Classification\n",
    "# -------------------------------\n",
    "X = features.astype('float32')\n",
    "y = tf.keras.utils.to_categorical(labels, 2)\n",
    "\n",
    "# Simple GCN model\n",
    "inputs = layers.Input(shape=X.shape[1])\n",
    "A_input = layers.Input(shape=(X.shape[0],), sparse=False)\n",
    "\n",
    "x = GCNConv(64, activation='relu')([inputs, A_input])\n",
    "x = GCNConv(32, activation='relu')([x, A_input])\n",
    "outputs = GCNConv(2, activation='softmax')([x, A_input])\n",
    "\n",
    "gcn_model = tf.keras.Model([inputs, A_input], outputs)\n",
    "gcn_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Train\n",
    "# -------------------------------\n",
    "gcn_model.fit([X, A], y, epochs=20, batch_size=X.shape[0], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, backend as K\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data Generators\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_gen = datagen.flow_from_directory(train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                        batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_gen = datagen.flow_from_directory(train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                      batch_size=BATCH_SIZE, class_mode='binary', subset='validation')\n",
    "test_gen = datagen.flow_from_directory(test_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                       batch_size=BATCH_SIZE, class_mode='binary', shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Capsule Network Layers\n",
    "# -------------------------------\n",
    "def squash(vectors, axis=-1):\n",
    "    s_squared_norm = K.sum(K.square(vectors), axis, keepdims=True)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return scale * vectors\n",
    "\n",
    "class CapsuleLayer(layers.Layer):\n",
    "    def __init__(self, num_capsules, dim_capsule, routings=3, **kwargs):\n",
    "        super(CapsuleLayer, self).__init__(**kwargs)\n",
    "        self.num_capsules = num_capsules\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(shape=[input_shape[1], self.num_capsules * self.dim_capsule],\n",
    "                                 initializer='glorot_uniform', trainable=True)\n",
    "        super(CapsuleLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        u_hat = K.dot(inputs, self.W)\n",
    "        u_hat = K.reshape(u_hat, (-1, inputs.shape[1], self.num_capsules, self.dim_capsule))\n",
    "        b = tf.zeros_like(u_hat[..., 0])\n",
    "        for i in range(self.routings):\n",
    "            c = tf.nn.softmax(b, axis=2)\n",
    "            s = tf.reduce_sum(c[..., None] * u_hat, axis=1)\n",
    "            v = squash(s)\n",
    "            if i < self.routings - 1:\n",
    "                b += tf.reduce_sum(u_hat * tf.expand_dims(v, 1), axis=-1)\n",
    "        return v\n",
    "\n",
    "# -------------------------------\n",
    "# Build the Model\n",
    "# -------------------------------\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = layers.Conv2D(64, kernel_size=5, strides=1, activation='relu')(inputs)\n",
    "x = layers.Conv2D(128, kernel_size=5, strides=1, activation='relu')(x)\n",
    "x = layers.Reshape((-1, 8))(x)  # Flatten into capsules\n",
    "caps = CapsuleLayer(num_capsules=10, dim_capsule=16, routings=3)(x)\n",
    "out = layers.Lambda(lambda z: K.sqrt(K.sum(K.square(z), 2)))(caps)\n",
    "out = layers.Dense(1, activation='sigmoid')(out)\n",
    "\n",
    "model = models.Model(inputs, out)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Train the Model\n",
    "# -------------------------------\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save Model\n",
    "# -------------------------------\n",
    "model.save(\"cats_dogs_capsnet.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fee233",
   "metadata": {},
   "source": [
    "## <b>Pretrained CNN / ViT Fine-Tuning<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61729a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# -------------------------------\n",
    "# Parameters\n",
    "# -------------------------------\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 20\n",
    "\n",
    "train_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\train\"\n",
    "test_dir = r\"C:\\Users\\ASUS\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\\test\"\n",
    "\n",
    "# -------------------------------\n",
    "# Data\n",
    "# -------------------------------\n",
    "datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "train_gen = datagen.flow_from_directory(train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                        batch_size=BATCH_SIZE, class_mode='binary', subset='training')\n",
    "val_gen = datagen.flow_from_directory(train_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                      batch_size=BATCH_SIZE, class_mode='binary', subset='validation')\n",
    "test_gen = datagen.flow_from_directory(test_dir, target_size=(IMG_SIZE, IMG_SIZE),\n",
    "                                       batch_size=BATCH_SIZE, class_mode='binary', shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# Attention-Augmented Conv Block\n",
    "# -------------------------------\n",
    "def attention_augmented_conv(x, filters, kernel_size, num_heads=4):\n",
    "    # Convolution\n",
    "    conv_out = layers.Conv2D(filters, kernel_size, padding='same', activation='relu')(x)\n",
    "    \n",
    "    # Flatten for attention\n",
    "    b, h, w, c = conv_out.shape\n",
    "    flatten = layers.Reshape((h*w, c))(conv_out)\n",
    "    \n",
    "    # Multi-head Self Attention\n",
    "    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=c)(flatten, flatten)\n",
    "    \n",
    "    # Reshape back to image\n",
    "    attn_out = layers.Reshape((h, w, c))(attn_out)\n",
    "    \n",
    "    # Add residual connection\n",
    "    out = layers.Add()([conv_out, attn_out])\n",
    "    return out\n",
    "\n",
    "# -------------------------------\n",
    "# Model\n",
    "# -------------------------------\n",
    "inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = attention_augmented_conv(inputs, 32, 3)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = attention_augmented_conv(x, 64, 3)\n",
    "x = layers.MaxPooling2D()(x)\n",
    "x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "model = models.Model(inputs, outputs)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# -------------------------------\n",
    "# Train\n",
    "# -------------------------------\n",
    "model.fit(train_gen, validation_data=val_gen, epochs=EPOCHS)\n",
    "\n",
    "# -------------------------------\n",
    "# Evaluate\n",
    "# -------------------------------\n",
    "test_loss, test_acc = model.evaluate(test_gen)\n",
    "print(f\"Test Accuracy: {test_acc*100:.2f}%\")\n",
    "\n",
    "# -------------------------------\n",
    "# Save Model\n",
    "# -------------------------------\n",
    "model.save(\"cats_dogs_attention_augmented.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97cddb1",
   "metadata": {},
   "source": [
    "| **Model**                                  | **Architecture / Type**                                         | **Strengths**                                                                         | **Weaknesses**                                                          | **Use Case**                                                     |\n",
    "| ------------------------------------------ | --------------------------------------------------------------- | ------------------------------------------------------------------------------------- | ----------------------------------------------------------------------- | ---------------------------------------------------------------- |\n",
    "| **Simple CNN**                             | Sequential CNN (Conv + Pool + FC)                               | Easy to implement, fast to train on small datasets                                    | Limited feature learning, may underfit complex images                   | Basic image classification tasks, educational purposes           |\n",
    "| **CNN + Transformer Hybrid**               | CNN backbone + Vision Transformer layers                        | Captures both local (CNN) and global (Transformer) features, better accuracy          | More complex, heavier, longer training                                  | Medium-scale image classification with structured patterns       |\n",
    "| **Vision Transformer (ViT)**               | Pure Transformer on image patches                               | Captures long-range dependencies, scales well with large datasets                     | Needs large datasets for good performance, computationally heavy        | Large-scale image classification, fine-grained image recognition |\n",
    "| **Self-Supervised Learning (SSL) CNN/ViT** | Pretext task (e.g., rotation prediction, contrastive learning)  | Learns powerful representations without labels, can improve downstream classification | Requires careful pretext design, slower training                        | Pretraining on unlabeled data, semi-supervised classification    |\n",
    "| **Graph-based Methods**                    | Images as nodes/features + GNN                                  | Captures relationships between samples, can incorporate structured info               | Less common for raw images, requires graph construction                 | Image classification with relational or structural data          |\n",
    "| **Capsule Networks (CapsNet)**             | Capsules + dynamic routing                                      | Preserves spatial hierarchies, robust to affine transformations                       | Complex, slower to train, less mature                                   | Small-scale datasets with complex spatial features               |\n",
    "| **NAS / Attention-Augmented CNNs**         | CNN optimized via Neural Architecture Search + attention layers | Potentially high accuracy, automatic architecture design                              | Very computationally expensive                                          | High-performance image classification when compute is available  |\n",
    "| **Pretrained CNN / ViT Fine-Tuning**       | ResNet, EfficientNet, or ViT pretrained on ImageNet             | High accuracy even on small datasets, fast convergence                                | Less flexible for completely new domains, may require domain adaptation | Transfer learning for medium-scale image classification          |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
