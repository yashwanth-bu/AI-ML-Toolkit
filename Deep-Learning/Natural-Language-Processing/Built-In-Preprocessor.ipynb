{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2b3a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4295ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [word.split() for word in [sentence[0].lower() for sentence in data]]\n",
    "\n",
    "unique_tokens = set([word for sentence in tokens for word in sentence])\n",
    "\n",
    "vocab = {word: idx for idx, word in enumerate(unique_tokens, start=2)}\n",
    "(vocab['PAD'], vocab['UNK']) = (0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5287ccd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "nlp_tokens = [[token.text for token in nlp(sentence[0]).doc] for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b2f1bc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "tt_tokens = [tokenizer.tokenize(sentence[0].lower()) for sentence in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab79de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([sentence[0].lower() for sentence in data])\n",
    "tk_tokens = tokenizer.texts_to_sequences([sentence[0].lower() for sentence in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a660609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(text):\n",
    "    return tokenizer.tokenize(text.lower())\n",
    "\n",
    "# Tokenize the sentences\n",
    "def tokenize_data(data):\n",
    "    return [(tokenize_function(sentence), label) for sentence, label in data]\n",
    "\n",
    "# Example usage:\n",
    "# Replace `data` with your actual dataset (list of tuples with sentences and labels)\n",
    "# tokenized_texts = tokenize_data(data)\n",
    "\n",
    "# For illustration, here's how you would print the tokenized sentences:\n",
    "# for sentence, tokens in tokenized_texts:\n",
    "#     print(f\"Original sentence: {sentence}\")\n",
    "#     print(f\"Tokenized: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4485842",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "'''\n",
    "TensorFlow's TextVectorization Layer: TensorFlow provides a TextVectorization layer, which is optimized for performance and integrates directly into TensorFlow models. This can be used for large datasets and avoids manually handling the vocabulary.\n",
    "'''\n",
    "\n",
    "\n",
    "# Create TextVectorization layer\n",
    "vectorizer = TextVectorization(max_tokens=10000, output_mode='int', output_sequence_length=100)\n",
    "\n",
    "# Fit the vectorizer\n",
    "vectorizer.adapt(texts)\n",
    "\n",
    "# Transform text to sequence of integers\n",
    "sequences = vectorizer(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2f0fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "# Set up the TextVectorization layer (you can add `oov_token` to handle unknown words)\n",
    "vectorizer = TextVectorization(\n",
    "    max_tokens=10000,         # Limit vocabulary size to 10,000 most frequent words\n",
    "    output_mode='int',        # Return sequences of integers\n",
    "    output_sequence_length=50,  # Ensure uniform sequence length (you can adjust this)\n",
    "    oov_token=\"<UNK>\"         # Handle out-of-vocabulary tokens\n",
    ")\n",
    "\n",
    "# Fit on the dataset (adapt to learn the vocabulary)\n",
    "vectorizer.adapt(flat_texts)\n",
    "\n",
    "# Transform text to sequences\n",
    "padded_sequences = vectorizer(flat_texts)\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976fe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def clean_data_parallel(data_batch):\n",
    "    \n",
    "    '''\n",
    "    Parallel Data Loading and Preprocessing\n",
    "\n",
    "    For very large datasets, consider parallelizing the data loading and preprocessing using libraries like joblib, concurrent.futures, or multiprocessing. This can significantly speed up the pipeline when processing a large volume of text data.\n",
    "\n",
    "    '''\n",
    "\n",
    "    return [clean_and_tokenize(text) for text in data_batch]\n",
    "\n",
    "# Split data into batches for parallel processing\n",
    "batch_size = 500\n",
    "batches = [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
    "\n",
    "# Use ProcessPoolExecutor to clean data in parallel\n",
    "with ProcessPoolExecutor() as executor:\n",
    "    results = list(executor.map(clean_data_parallel, batches))\n",
    "\n",
    "# Flatten the list of results\n",
    "cleaned_texts = [item for sublist in results for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2435c146",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Prepare data (lowercase text)\n",
    "texts = [sentence[0].lower() for sentence in data]\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")  # For handling out-of-vocabulary words\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Create vocabulary (words mapped to integer indices)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add PAD token at index 0\n",
    "vocab['PAD'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7560f05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]\n",
    "\n",
    "# Prepare text data (extract only the sentences and convert to lowercase)\n",
    "texts = [sentence[0].lower() for sentence in data]\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = Tokenizer(oov_token=\"<UNK>\")  # Out-of-vocabulary token\n",
    "\n",
    "# Fit the tokenizer on your text data (builds the vocabulary)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "# Create the vocabulary (words -> index mapping)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add padding (for consistency with your original approach)\n",
    "vocab['PAD'] = 0  # We define PAD as 0, to match your earlier setup\n",
    "\n",
    "# Show the vocabulary (word -> index mapping)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Optionally, convert the texts into sequences (word indices)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "# Show sequences (list of word indices)\n",
    "print(\"Sequences:\", sequences)\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Pad the sequences so that they all have the same length\n",
    "padded_sequences = pad_sequences(sequences, padding='post')\n",
    "\n",
    "print(\"Padded Sequences:\", padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74cb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #with spacy\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences as padding\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]\n",
    "\n",
    "# Function to clean and tokenize text using spaCy\n",
    "def clean_and_tokenize(text):\n",
    "    doc = nlp(text.lower())  # Process the text with spaCy (lowercase, tokenization)\n",
    "    # Remove stop words and punctuation, keep only alphanumeric tokens\n",
    "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Clean and tokenize the text data\n",
    "texts = [clean_and_tokenize(sentence[0]) for sentence in data]\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "# This will ensure that the vocabulary is limited to the 10,000 most frequent words.\n",
    "tokenizer = tokenizer(num_words=10000, oov_token=\"<UNK>\")  # Out-of-vocabulary token\n",
    "\n",
    "# Fit the tokenizer on your cleaned text data\n",
    "flat_texts = [\" \".join(sentence) for sentence in texts]  # Join words back into a sentence\n",
    "tokenizer.fit_on_texts(flat_texts)\n",
    "\n",
    "# Create the vocabulary (word -> index mapping)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add padding token with index 0\n",
    "vocab['PAD'] = 0\n",
    "\n",
    "# Show the vocabulary (word -> index mapping)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert cleaned and tokenized sentences into sequences (word indices)\n",
    "sequences = tokenizer.texts_to_sequences(flat_texts)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "padded_sequences = padding(sequences, padding='post')\n",
    "\n",
    "# Show sequences and padded sequences\n",
    "print(\"Sequences:\", sequences)\n",
    "print(\"Padded Sequences:\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0444a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #ngrams\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences as padding\n",
    "from nltk.util import ngrams\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]\n",
    "\n",
    "# Function to clean and tokenize text using spaCy\n",
    "def clean_and_tokenize(text):\n",
    "    doc = nlp(text.lower())  # Process the text with spaCy (lowercase, tokenization)\n",
    "    # Remove stop words and punctuation, keep only alphanumeric tokens\n",
    "    return [token.text for token in doc if not token.is_stop and not token.is_punct]\n",
    "\n",
    "# Function to generate n-grams (bigrams, trigrams, etc.)\n",
    "def generate_ngrams(tokens, n=2):\n",
    "    return [' '.join(gram) for gram in ngrams(tokens, n)]\n",
    "\n",
    "# Clean and tokenize the text data\n",
    "texts = [clean_and_tokenize(sentence[0]) for sentence in data]\n",
    "\n",
    "# Generate n-grams (for example, bigrams)\n",
    "n = 2  # For bigrams, change to 3 for trigrams, etc.\n",
    "texts_with_ngrams = []\n",
    "\n",
    "for text in texts:\n",
    "    # Generate n-grams and append to the tokenized sentence\n",
    "    ngram_tokens = generate_ngrams(text, n)\n",
    "    texts_with_ngrams.append(text + ngram_tokens)  # Combine words and n-grams\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = tokenizer(oov_token=\"<UNK>\")  # Out-of-vocabulary token\n",
    "\n",
    "# Fit the tokenizer on your cleaned text data with n-grams\n",
    "flat_texts = [\" \".join(sentence) for sentence in texts_with_ngrams]  # Join words and n-grams back into a sentence\n",
    "tokenizer.fit_on_texts(flat_texts)\n",
    "\n",
    "# Create the vocabulary (word -> index mapping)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add padding token with index 0\n",
    "vocab['PAD'] = 0\n",
    "\n",
    "# Show the vocabulary (word -> index mapping)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert cleaned and tokenized sentences with n-grams into sequences (word indices)\n",
    "sequences = tokenizer.texts_to_sequences(flat_texts)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "padded_sequences = padding(sequences, padding='post')\n",
    "\n",
    "# Show sequences and padded sequences\n",
    "print(\"Sequences:\", sequences)\n",
    "print(\"Padded Sequences:\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773e55c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #parts of speech tagger\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences as padding\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]\n",
    "\n",
    "# Function to clean, tokenize, and add POS tags using spaCy\n",
    "def clean_and_tokenize_with_pos(text):\n",
    "    doc = nlp(text.lower())  # Process the text with spaCy (lowercase, tokenization)\n",
    "    # Extract tokens and their POS tags (filter out stop words and punctuation)\n",
    "    tokens_pos = [(token.text, token.pos_) for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return tokens_pos\n",
    "\n",
    "# Clean and tokenize the text data with POS tags\n",
    "texts_with_pos = [clean_and_tokenize_with_pos(sentence[0]) for sentence in data]\n",
    "\n",
    "# Show tokenized text with POS tags\n",
    "print(\"Tokenized and POS-tagged Texts:\", texts_with_pos)\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = tokenizer(oov_token=\"<UNK>\")  # Out-of-vocabulary token\n",
    "\n",
    "# Prepare the text data (flatten the list of tokenized words)\n",
    "flat_texts = [\" \".join([token[0] for token in sentence]) for sentence in texts_with_pos]  # Only words, not POS tags\n",
    "\n",
    "# Fit the tokenizer on your cleaned text data\n",
    "tokenizer.fit_on_texts(flat_texts)\n",
    "\n",
    "# Create the vocabulary (word -> index mapping)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add padding token with index 0\n",
    "vocab['PAD'] = 0\n",
    "\n",
    "# Show the vocabulary (word -> index mapping)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert cleaned and tokenized sentences into sequences (word indices)\n",
    "sequences = tokenizer.texts_to_sequences(flat_texts)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "padded_sequences = padding(sequences, padding='post')\n",
    "\n",
    "# Show sequences and padded sequences\n",
    "print(\"Sequences:\", sequences)\n",
    "print(\"Padded Sequences:\", padded_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1596319e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy #name entity recognizer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer as tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences as padding\n",
    "\n",
    "# Load spaCy's small English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Your data\n",
    "data = [\n",
    "    (\"I love this movie\", 1),          \n",
    "    (\"This film is terrible\", 0), \n",
    "    (\"What a great movie\", 1),\n",
    "    (\"I hated this film\", 0),\n",
    "    (\"Amazing acting and good story\", 1),\n",
    "    (\"Bad plot and boring\", 0),\n",
    "]\n",
    "\n",
    "# Function to clean, tokenize, add POS tags, and extract named entities using spaCy\n",
    "def clean_and_tokenize_with_ner(text):\n",
    "    doc = nlp(text.lower())  # Process the text with spaCy (lowercase, tokenization)\n",
    "    tokens = [(token.text, token.pos_) for token in doc if not token.is_stop and not token.is_punct]\n",
    "    # Extract named entities\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]  # (Entity text, Entity type)\n",
    "    return tokens, entities\n",
    "\n",
    "# Clean, tokenize, and extract NER for each sentence\n",
    "texts_with_pos_and_entities = [clean_and_tokenize_with_ner(sentence[0]) for sentence in data]\n",
    "\n",
    "# Show tokenized text with POS tags and Named Entities\n",
    "print(\"Tokenized Texts with POS and Named Entities:\", texts_with_pos_and_entities)\n",
    "\n",
    "# Initialize the Tokenizer\n",
    "tokenizer = tokenizer(oov_token=\"<UNK>\")  # Out-of-vocabulary token\n",
    "\n",
    "# Prepare the text data (flatten the list of tokenized words)\n",
    "flat_texts = [\" \".join([token[0] for token in sentence[0]]) for sentence in texts_with_pos_and_entities]  # Only words, not POS tags or entities\n",
    "\n",
    "# Fit the tokenizer on your cleaned text data\n",
    "tokenizer.fit_on_texts(flat_texts)\n",
    "\n",
    "# Create the vocabulary (word -> index mapping)\n",
    "vocab = tokenizer.word_index\n",
    "\n",
    "# Add padding token with index 0\n",
    "vocab['PAD'] = 0\n",
    "\n",
    "# Show the vocabulary (word -> index mapping)\n",
    "print(\"Vocabulary:\", vocab)\n",
    "\n",
    "# Convert cleaned and tokenized sentences into sequences (word indices)\n",
    "sequences = tokenizer.texts_to_sequences(flat_texts)\n",
    "\n",
    "# Pad the sequences so they all have the same length\n",
    "padded_sequences = padding(sequences, padding='post')\n",
    "\n",
    "# Show sequences and padded sequences\n",
    "print(\"Sequences:\", sequences)\n",
    "print(\"Padded Sequences:\", padded_sequences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
